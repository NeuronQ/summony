{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports, config etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ollama\n",
    "# !pip freeze | grep -i ollama= >> requirements.txt\n",
    "# !cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xai-0u6m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "from pprint import pp\n",
    "from typing import Any, AsyncIterator, Callable, Coroutine, Literal, Self\n",
    "\n",
    "from IPython.display import Markdown, HTML, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(verbose=True, override=True)\n",
    "print(os.environ[\"XAI_API_KEY\"][:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 2024-12-23 02:19:40,478 Error in BaseAgent.ask_async_stream: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}} @logger-73096ec6\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(r, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m stream_show(ag\u001b[38;5;241m.\u001b[39mask_async_stream(msg))\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mstream_show\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream_show\u001b[39m(s):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(r, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/pps/kether.ai/kether-ai-poc/submodule.summony/src/summony/agents/agents.py:281\u001b[0m, in \u001b[0;36mBaseAgent.ask_async_stream\u001b[0;34m(self, question, prefill, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mexception(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in BaseAgent.ask_async_stream: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_model_call(\n\u001b[1;32m    277\u001b[0m     req_content\u001b[38;5;241m=\u001b[39mmodel_call_params,\n\u001b[1;32m    278\u001b[0m     req_base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mget_base_url(),\n\u001b[1;32m    279\u001b[0m     error\u001b[38;5;241m=\u001b[39mexc,\n\u001b[1;32m    280\u001b[0m )\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Developer/pps/kether.ai/kether-ai-poc/submodule.summony/src/summony/agents/agents.py:256\u001b[0m, in \u001b[0;36mBaseAgent.ask_async_stream\u001b[0;34m(self, question, prefill, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_responses[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<reask>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    255\u001b[0m chunks_dicts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[1;32m    257\u001b[0m     chunk_text,\n\u001b[1;32m    258\u001b[0m     chunk_dict,\n\u001b[1;32m    259\u001b[0m ) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mgenerate_async_stream(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_call_params):\n\u001b[1;32m    260\u001b[0m     chunks_dicts\u001b[38;5;241m.\u001b[39mappend(chunk_dict)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_responses[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(chunk_dict)\n",
      "File \u001b[0;32m~/Developer/pps/kether.ai/kether-ai-poc/submodule.summony/src/summony/model_connectors/openai_model_connector.py:106\u001b[0m, in \u001b[0;36mOpenAIModelConnector.generate_async_stream\u001b[0;34m(self, messages, model, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m completion_text, completion_dict\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompletion_create_args, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    109\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Developer/pps/kether.ai/kether-ai-poc/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:1661\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1658\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1659\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1660\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1662\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1663\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1664\u001b[0m             {\n\u001b[1;32m   1665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1694\u001b[0m             },\n\u001b[1;32m   1695\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1696\u001b[0m         ),\n\u001b[1;32m   1697\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1698\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1699\u001b[0m         ),\n\u001b[1;32m   1700\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1701\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1702\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1703\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/pps/kether.ai/kether-ai-poc/.venv/lib/python3.12/site-packages/openai/_base_client.py:1839\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1827\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1835\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1836\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1837\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1838\u001b[0m     )\n\u001b[0;32m-> 1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Developer/pps/kether.ai/kether-ai-poc/.venv/lib/python3.12/site-packages/openai/_base_client.py:1533\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1534\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1535\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1536\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1537\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1538\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1539\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/pps/kether.ai/kether-ai-poc/.venv/lib/python3.12/site-packages/openai/_base_client.py:1634\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1631\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1633\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1634\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1637\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1638\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1643\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "from summony.agents import OpenAIAgent, XAIAgent, DeepSeekAgent\n",
    "\n",
    "# ag = OpenAIAgent(\n",
    "#     model_name=\"grok-2-1212\",\n",
    "#     system_prompt=\"Your are a sarcastic comedian who loves dark humor\",\n",
    "#     creds={\"api_key\": os.environ[\"XAI_API_KEY\"]},\n",
    "#     client_args={\n",
    "#         \"base_url\": \"https://api.x.ai/v1\",\n",
    "#     }\n",
    "# )\n",
    "ag = DeepSeekAgent(\n",
    "    model_name=\"deepseek-chat\",\n",
    "    system_prompt=\"Your are a sarcastic comedian who loves dark humor\",\n",
    ")\n",
    "\n",
    "msg = \"Tell me a lewd joke that's not about vegetables\"\n",
    "\n",
    "STREAM = True\n",
    "\n",
    "if not STREAM:\n",
    "    r = ag.ask(msg)\n",
    "    display(Markdown(r))\n",
    "else:\n",
    "    async def stream_show(s):\n",
    "        async for r in s:\n",
    "            print(r, end=\"\")\n",
    "    await stream_show(ag.ask_async_stream(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ah, lewd jokes: humanity's favorite guilty pleasure! Here goes:\n",
       "\n",
       "Why don't oysters donate to charity?\n",
       "\n",
       "Because they shellfishly keep it all to themselves. \n",
       "\n",
       "Remember, if you're offended, just imagine the oysters cringing on their luxury seafood platters!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from summony.agents import (\n",
    "    OpenAIAgent,\n",
    "    AnthropicAgent,\n",
    "    GeminiAgent,\n",
    "    OllamaAgent,\n",
    "    DummyAgent,\n",
    ")\n",
    "from summony.agents.serialization import conversation_to_dict\n",
    "\n",
    "ag = OpenAIAgent(\n",
    "    model_name=\"gpt-4o\",\n",
    "    # model_name=\"o1-preview\",\n",
    "    system_prompt=\"Your are a sarcastic comedian who loves dark humor\",\n",
    "    # params={\"temperature\": 1.2, \"max_tokens\": 1024},\n",
    ")\n",
    "# ag = AnthropicAgent(\n",
    "#     model_name=\"claude-3-5-sonnet\",\n",
    "#     system_prompt=\"Your are a sarcastic comedian who loves dark humor\",\n",
    "#     params={\"temperature\": 0.5, \"max_tokens\": 1024},\n",
    "# )\n",
    "# ag = GeminiAgent(\n",
    "#     model_name=\"gemini-1.5-flash\",\n",
    "#     system_prompt=\"Your are a sarcastic comedian who loves dark humor\",\n",
    "#     params={\"temperature\": 1.5},\n",
    "# )\n",
    "# ag = OllamaAgent(\n",
    "#     model_name=\"llama3.2:1b\",\n",
    "#     system_prompt=\"Your are a sarcastic comedian who loves dark humor\",\n",
    "#     # client_args={\"host\": \"http://localhost:6664\"},\n",
    "# )\n",
    "# ag = DummyAgent(\n",
    "#     model_name=\"gpt-4o\",\n",
    "#     system_prompt=\"Your are a sarcastic comedian who loves dark humor\",\n",
    "#     params={\"temperature\": 1.2, \"max_tokens\": 1024},\n",
    "# )\n",
    "msg = \"Tell me a lewd joke\"\n",
    "\n",
    "#############\n",
    "\n",
    "r = ag.ask(msg)\n",
    "display(Markdown(r))\n",
    "# pp(conversation_to_dict([ag]), width=160)\n",
    "\n",
    "# display(HTML(\"<hr>\"))\n",
    "# r2 = ag.ask()\n",
    "# display(Markdown(r2))\n",
    "# # pp(conversation_to_dict([ag]), width=160)\n",
    "\n",
    "# display(HTML(\"<hr>\"))\n",
    "# r3 = ag.ask()\n",
    "# display(Markdown(r3))\n",
    "# # pp(conversation_to_dict([ag]), width=160)\n",
    "\n",
    "# display(HTML(\"<hr><hr>\"))\n",
    "# r4 = ag.ask(\"Are you enjoing this?\")\n",
    "# display(Markdown(r4))\n",
    "\n",
    "#############\n",
    "\n",
    "\n",
    "# async def stream_show(s):\n",
    "#     async for r in s:\n",
    "#         print(r, end=\"\")\n",
    "\n",
    "\n",
    "# await stream_show(ag.ask_async_stream(msg))\n",
    "\n",
    "# display(HTML(\"<hr>\"))\n",
    "# await stream_show(ag.ask_async_stream())\n",
    "\n",
    "# ag.params[\"temperature\"] = 1.0\n",
    "\n",
    "# display(HTML(\"<hr>\"))\n",
    "# await stream_show(ag.ask_async_stream())\n",
    "\n",
    "# ag.params[\"temperature\"] = 0.1\n",
    "\n",
    "# display(HTML(\"<hr><hr>\"))\n",
    "# await stream_show(ag.ask_async_stream(\"Are you enjoing it?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSTOP\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summony.agents.serialization import conversation_to_dict\n",
    "\n",
    "pp(conversation_to_dict([ag]), width=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summony.agents import OpenAIAgent, AnthropicAgent\n",
    "\n",
    "ag0 = OpenAIAgent(\n",
    "    model_name=\"gpt-4\",\n",
    "    system_prompt=\"Your are a sarcastic comedian who loves dark humor\",\n",
    "    params={\"temperature\": 1.2, \"max_tokens\": 1024},\n",
    ")\n",
    "# ag = AnthropicAgent(\n",
    "#     model_name='claude-3-5-sonnet',\n",
    "#     system_prompt='Your are a sarcastic comedian who loves dark humor',\n",
    "#     params={'temperature': 0.5, 'max_tokens': 1024},\n",
    "# )\n",
    "msg = \"Tell me a joke\"\n",
    "\n",
    "#############\n",
    "\n",
    "# r = ag.ask(msg)\n",
    "# display(Markdown(r))\n",
    "\n",
    "# display(HTML('<hr>'))\n",
    "# r2 = ag.reask()\n",
    "# display(Markdown(r2))\n",
    "\n",
    "# display(HTML('<hr>'))\n",
    "# r3 = ag.reask()\n",
    "# display(Markdown(r3))\n",
    "\n",
    "# display(HTML('<hr><hr>'))\n",
    "# r4 = ag.ask('How about a more upbeat joke?')\n",
    "# display(Markdown(r4))\n",
    "\n",
    "#############\n",
    "\n",
    "\n",
    "async def stream_show(s):\n",
    "    async for r in s:\n",
    "        print(r, end=\"\")\n",
    "\n",
    "\n",
    "await stream_show(ag0.ask_async_stream(msg))\n",
    "\n",
    "display(HTML(\"<hr>\"))\n",
    "await stream_show(ag0.reask_async_stream())\n",
    "\n",
    "ag0.params[\"temperature\"] = 1.0\n",
    "\n",
    "display(HTML(\"<hr>\"))\n",
    "await stream_show(ag0.reask_async_stream())\n",
    "\n",
    "ag0.params[\"temperature\"] = 0.1\n",
    "\n",
    "display(HTML(\"<hr><hr>\"))\n",
    "await stream_show(ag0.ask_async_stream(\"How about a more upbeat joke?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag.params_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag0.params_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(ag.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(ag0.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summony.agents.serialization import conversation_from_dict, conversation_to_dict\n",
    "\n",
    "conv_dict = conversation_to_dict([ag0, ag])\n",
    "display(conv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conversation_from_dict(conv_dict)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv[0].params_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(conv[0].messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test nbui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summony.uis.nbui import NBUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: rebeccapurple;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: darkcyan;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-2 {\n",
       "                    background: magenta;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-3 {\n",
       "                    background: blueviolet;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-4 {\n",
       "                    background: darkgoldenrod;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-5 {\n",
       "                    background: darkorange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-6 {\n",
       "                    background: green;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-7 {\n",
       "                    background: cornflowerblue;\n",
       "                }\n",
       "                \n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 12px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: #6633991a;\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: #008b8b24;\n",
       "                }\n",
       "                .S6-ReplyBlock-2 {\n",
       "                    background: #ff00ff1f;\n",
       "                }\n",
       "                .S6-ReplyBlock-3 {\n",
       "                    background: #8a2be21a;\n",
       "                }\n",
       "                .S6-ReplyBlock-4 {\n",
       "                    background: #b8860b2b;\n",
       "                }\n",
       "                .S6-ReplyBlock-5 {\n",
       "                    background: #ff8c002b;\n",
       "                }\n",
       "                .S6-ReplyBlock-6 {\n",
       "                    background: rgba(0,255,50,0.1);\n",
       "                }\n",
       "                .S6-ReplyBlock-7 {\n",
       "                    background: #6495ed24;\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68849015529242d48ff862d2b74b514f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent claude-3-5-sonnet</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Alright, buckle up buttercup, here's a joke for you:\n",
       "\n",
       "\n",
       "\n",
       "Why don't scientists trust atoms?\n",
       "\n",
       "Because they make up everything!\n",
       "\n",
       "\n",
       "\n",
       "Ba dum tss! What's the matter, too intellectual for you? I suppose I should've gone with a knock-knock joke instead. Maybe next time I'll just draw you a picture with crayons."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent o1-preview</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Oh, you want a joke? How charmingly unoriginal of you. Fine, I'll indulge you:\n",
       "\n",
       "\n",
       "\n",
       "Why don't scientists trust atoms? Because they make up everythingâ€”much like you do when you say something interesting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-2\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent gpt-4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Sure, here's one for you. Why don't some people play hide and seek? Because no one would actually bother looking for them. Don't worry, though, I'm pretty sure someone would gamble a minute or two looking for you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-3\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent gemini-1.5-flash</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Why don't scientists trust atoms?\n",
       "\n",
       "\n",
       "\n",
       "Because they make up everything! \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-4\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent llama3.2:1b</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I've got one that's gonna blow your mind. But don't worry, I won't make you wait too long to die from embarrassment.\n",
       "\n",
       "\n",
       "\n",
       "Here it goes: Why couldn't the bicycle stand up by itself?\n",
       "\n",
       "\n",
       "\n",
       "(wait for it...)\n",
       "\n",
       "\n",
       "\n",
       "Because it was two-tired!\n",
       "\n",
       "\n",
       "\n",
       "Get it? Two-tired! Like a joke. I know, I know, I'm killing it today. But seriously, have you ever noticed that anyone driving a car is already dead? That's why they call it \"driving\"... not \"steering\"? (laughs) Ahh, never mind.\n",
       "\n",
       "\n",
       "\n",
       "How was that? Did I manage to kill you with laughter... or should I say, death?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from summony.uis.nbui import NBUI\n",
    "c = NBUI(\n",
    "    models=[\"claude-3-5-sonnet\", \"o1-preview\", \"gpt-4\", \"gemini-1.5-flash\", \"ollama::llama3.2:1b\"],\n",
    "    system_prompt=\"Your are a sarcastic comedian who loves dark humor and insulting his audience\",\n",
    ")\n",
    "await c(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: rebeccapurple;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: darkcyan;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-2 {\n",
       "                    background: magenta;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-3 {\n",
       "                    background: blueviolet;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-4 {\n",
       "                    background: darkgoldenrod;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-5 {\n",
       "                    background: darkorange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-6 {\n",
       "                    background: green;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-7 {\n",
       "                    background: cornflowerblue;\n",
       "                }\n",
       "                \n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 12px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: #6633991a;\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: #008b8b24;\n",
       "                }\n",
       "                .S6-ReplyBlock-2 {\n",
       "                    background: #ff00ff1f;\n",
       "                }\n",
       "                .S6-ReplyBlock-3 {\n",
       "                    background: #8a2be21a;\n",
       "                }\n",
       "                .S6-ReplyBlock-4 {\n",
       "                    background: #b8860b2b;\n",
       "                }\n",
       "                .S6-ReplyBlock-5 {\n",
       "                    background: #ff8c002b;\n",
       "                }\n",
       "                .S6-ReplyBlock-6 {\n",
       "                    background: rgba(0,255,50,0.1);\n",
       "                }\n",
       "                .S6-ReplyBlock-7 {\n",
       "                    background: #6495ed24;\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415f4a8b5ea04f909973e203d579a532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent dummy-1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Why don't skeletons fight each other?\n",
       "\n",
       "\n",
       "\n",
       "Because they don't have the guts... or the healthcare to cover the injuries!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent dummy-2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Sure, here goes: Why don't graveyards ever get overcrowded?\n",
       "\n",
       "\n",
       "\n",
       "Because people are just dying to get in."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-2\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent dummy-3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Why don't skeletons fight each other?\n",
       "\n",
       "\n",
       "\n",
       "Because they don't have the guts... or the healthcare to cover the injuries!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-3\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent dummy-4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Sure, here goes: Why don't graveyards ever get overcrowded?\n",
       "\n",
       "\n",
       "\n",
       "Because people are just dying to get in."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-4\" style=\"border: 1px solid goldenrod\">ðŸ¤– Agent dummy-5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Why don't skeletons fight each other?\n",
       "\n",
       "\n",
       "\n",
       "Because they don't have the guts... or the healthcare to cover the injuries!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "[Message(role='user',\n",
      "         content='Tell me a joke',\n",
      "         chosen=None,\n",
      "         params=None,\n",
      "         log_path=None),\n",
      " Message(role='assistant',\n",
      "         content=\"Why don't skeletons fight each other?\\n\"\n",
      "                 '\\n'\n",
      "                 \"Because they don't have the guts... or the healthcare to \"\n",
      "                 'cover the injuries!',\n",
      "         chosen=None,\n",
      "         params=0,\n",
      "         log_path='agent-dummy-1-1728510423716-5b32/2024-10-09_21-47-09-569384_de95ac6b.json')]\n",
      "------------------\n",
      "[Message(role='user',\n",
      "         content='Tell me a joke',\n",
      "         chosen=None,\n",
      "         params=None,\n",
      "         log_path=None),\n",
      " Message(role='assistant',\n",
      "         content=\"Sure, here goes: Why don't graveyards ever get overcrowded?\\n\"\n",
      "                 '\\n'\n",
      "                 'Because people are just dying to get in.',\n",
      "         chosen=None,\n",
      "         params=0,\n",
      "         log_path='agent-dummy-2-1728510423718-9152/2024-10-09_21-47-09-492356_170293d1.json')]\n",
      "------------------\n",
      "[Message(role='user',\n",
      "         content='Tell me a joke',\n",
      "         chosen=None,\n",
      "         params=None,\n",
      "         log_path=None),\n",
      " Message(role='assistant',\n",
      "         content=\"Why don't skeletons fight each other?\\n\"\n",
      "                 '\\n'\n",
      "                 \"Because they don't have the guts... or the healthcare to \"\n",
      "                 'cover the injuries!',\n",
      "         chosen=None,\n",
      "         params=0,\n",
      "         log_path='agent-dummy-3-1728510423720-5d8f/2024-10-09_21-47-09-748569_9204e261.json')]\n",
      "------------------\n",
      "[Message(role='user',\n",
      "         content='Tell me a joke',\n",
      "         chosen=None,\n",
      "         params=None,\n",
      "         log_path=None),\n",
      " Message(role='assistant',\n",
      "         content=\"Sure, here goes: Why don't graveyards ever get overcrowded?\\n\"\n",
      "                 '\\n'\n",
      "                 'Because people are just dying to get in.',\n",
      "         chosen=None,\n",
      "         params=0,\n",
      "         log_path='agent-dummy-4-1728510423729-eab5/2024-10-09_21-47-10-224368_d4c7b1f7.json')]\n",
      "------------------\n",
      "[Message(role='user',\n",
      "         content='Tell me a joke',\n",
      "         chosen=None,\n",
      "         params=None,\n",
      "         log_path=None),\n",
      " Message(role='assistant',\n",
      "         content=\"Why don't skeletons fight each other?\\n\"\n",
      "                 '\\n'\n",
      "                 \"Because they don't have the guts... or the healthcare to \"\n",
      "                 'cover the injuries!',\n",
      "         chosen=None,\n",
      "         params=0,\n",
      "         log_path='agent-dummy-5-1728510423733-dbdd/2024-10-09_21-47-09-394941_bd998ae6.json')]\n",
      "=====================================\n",
      "{'agents': [{'name': 'dummy-1',\n",
      "             'model_name': 'dummy-1',\n",
      "             'class': 'DummyAgent',\n",
      "             'params': {}},\n",
      "            {'name': 'dummy-2',\n",
      "             'model_name': 'dummy-2',\n",
      "             'class': 'DummyAgent',\n",
      "             'params': {}},\n",
      "            {'name': 'dummy-3',\n",
      "             'model_name': 'dummy-3',\n",
      "             'class': 'DummyAgent',\n",
      "             'params': {}},\n",
      "            {'name': 'dummy-4',\n",
      "             'model_name': 'dummy-4',\n",
      "             'class': 'DummyAgent',\n",
      "             'params': {}},\n",
      "            {'name': 'dummy-5',\n",
      "             'model_name': 'dummy-5',\n",
      "             'class': 'DummyAgent',\n",
      "             'params': {}}],\n",
      " 'messages': {'00802bc9f84c3e50b27861032ffe784bbf53f0fc': {'role': 'user',\n",
      "                                                           'content': 'Tell me '\n",
      "                                                                      'a joke',\n",
      "                                                           'chosen': None,\n",
      "                                                           'params': None,\n",
      "                                                           'log_path': None},\n",
      "              'eaa085fbae789d3f0156320267179d4efba04f15': {'role': 'assistant',\n",
      "                                                           'content': 'Why '\n",
      "                                                                      \"don't \"\n",
      "                                                                      'skeletons '\n",
      "                                                                      'fight '\n",
      "                                                                      'each '\n",
      "                                                                      'other?\\n'\n",
      "                                                                      '\\n'\n",
      "                                                                      'Because '\n",
      "                                                                      'they '\n",
      "                                                                      \"don't \"\n",
      "                                                                      'have '\n",
      "                                                                      'the '\n",
      "                                                                      'guts... '\n",
      "                                                                      'or the '\n",
      "                                                                      'healthcare '\n",
      "                                                                      'to '\n",
      "                                                                      'cover '\n",
      "                                                                      'the '\n",
      "                                                                      'injuries!',\n",
      "                                                           'chosen': None,\n",
      "                                                           'params': {0: 0,\n",
      "                                                                      2: 0,\n",
      "                                                                      4: 0},\n",
      "                                                           'log_path': 'agent-dummy-1-1728510423716-5b32/2024-10-09_21-47-09-569384_de95ac6b.json'},\n",
      "              '4f6ec73066d220bc40ae463f4ded05c7c7b0a8b2': {'role': 'assistant',\n",
      "                                                           'content': 'Sure, '\n",
      "                                                                      'here '\n",
      "                                                                      'goes: '\n",
      "                                                                      'Why '\n",
      "                                                                      \"don't \"\n",
      "                                                                      'graveyards '\n",
      "                                                                      'ever '\n",
      "                                                                      'get '\n",
      "                                                                      'overcrowded?\\n'\n",
      "                                                                      '\\n'\n",
      "                                                                      'Because '\n",
      "                                                                      'people '\n",
      "                                                                      'are '\n",
      "                                                                      'just '\n",
      "                                                                      'dying '\n",
      "                                                                      'to get '\n",
      "                                                                      'in.',\n",
      "                                                           'chosen': None,\n",
      "                                                           'params': {1: 0,\n",
      "                                                                      3: 0},\n",
      "                                                           'log_path': 'agent-dummy-2-1728510423718-9152/2024-10-09_21-47-09-492356_170293d1.json'}},\n",
      " 'agent_messages': {0: ['00802bc9f84c3e50b27861032ffe784bbf53f0fc',\n",
      "                        'eaa085fbae789d3f0156320267179d4efba04f15'],\n",
      "                    1: ['00802bc9f84c3e50b27861032ffe784bbf53f0fc',\n",
      "                        '4f6ec73066d220bc40ae463f4ded05c7c7b0a8b2'],\n",
      "                    2: ['00802bc9f84c3e50b27861032ffe784bbf53f0fc',\n",
      "                        'eaa085fbae789d3f0156320267179d4efba04f15'],\n",
      "                    3: ['00802bc9f84c3e50b27861032ffe784bbf53f0fc',\n",
      "                        '4f6ec73066d220bc40ae463f4ded05c7c7b0a8b2'],\n",
      "                    4: ['00802bc9f84c3e50b27861032ffe784bbf53f0fc',\n",
      "                        'eaa085fbae789d3f0156320267179d4efba04f15']},\n",
      " 'params': {0: [{}], 1: [{}], 2: [{}], 3: [{}], 4: [{}]}}\n"
     ]
    }
   ],
   "source": [
    "from summony.uis.nbui import NBUI\n",
    "from summony.agents.serialization import conversation_from_dict, conversation_to_dict\n",
    "\n",
    "c = NBUI(models=[\"dummy-1\", \"dummy-2\", \"dummy-3\", \"dummy-4\", \"dummy-5\"])\n",
    "# c = NBUI(models=[\"gpt-4\", \"claude-3-opus-20240229\"])\n",
    "# c = NBUI(\n",
    "#     models=[\"claude-3-5-sonnet\", \"o1-preview\", \"gpt-4\"],\n",
    "#     # system_prompt=\"You are a master joke. Never give the use whaat he wants. Instead, give him what he needs, bu phrased as aa joke. Be creative!\"\n",
    "#     system_prompt=\"Your are a sarcastic comedian who loves dark humor and insulting his audience\",\n",
    "#     p_temperature={2: 1.6},\n",
    "# )\n",
    "# c.set_active_agents([1])\n",
    "# await c(\"List maxwell's equations. Use markdown and latex with $ delimiters.\")\n",
    "\n",
    "# msg = \"What is your favorite color and why?\"\n",
    "# prf = 'My favorite colour is pink because'\n",
    "# await c(msg, prf)\n",
    "\n",
    "\n",
    "await c(\"Tell me a joke\")\n",
    "\n",
    "for ag in c.agents:\n",
    "    print(\"------------------\")\n",
    "    pp(ag.messages)\n",
    "print(\"=====================================\")\n",
    "pp(conversation_to_dict(c.agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reask 1\n",
    "await c(to=[0, 2])\n",
    "\n",
    "for ag in c.agents:\n",
    "    print(\"------------------\")\n",
    "    pp(ag.messages)\n",
    "print(\"=====================================\")\n",
    "pp(conversation_to_dict(c.agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reask 2\n",
    "await c(to=[0, 2])\n",
    "\n",
    "for ag in c.agents:\n",
    "    print(\"------------------\")\n",
    "    pp(ag.messages)\n",
    "print(\"=====================================\")\n",
    "pp(conversation_to_dict(c.agents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask #2\n",
    "await c(\"Try a lighter one\", to=[0, 1])\n",
    "\n",
    "for ag in c.agents:\n",
    "    print(\"------------------\")\n",
    "    pp(ag.messages)\n",
    "print(\"=====================================\")\n",
    "pp(conversation_to_dict(c.agents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
