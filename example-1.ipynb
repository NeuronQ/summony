{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports, config etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pprint import pp\n",
    "from typing import Any, AsyncIterator, Callable, Coroutine, Literal, Self\n",
    "\n",
    "from IPython.display import Markdown, HTML, display, JSON\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summony.uis.nbui import NBUI\n",
    "from summony.agents.serialization import conversation_from_dict, conversation_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb8548705334880a3e6e369d610b857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\">ü§ñ Agent gpt-4o</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Entropy is a multifaceted concept that is foundational in various fields, particularly in thermodynamics, statistical mechanics, information theory, and even philosophy. Here, I will outline the concept of entropy in different contexts:\n",
       "\n",
       "\n",
       "\n",
       "### Thermodynamics\n",
       "\n",
       "\n",
       "\n",
       "In classical thermodynamics, entropy is a measure of the amount of energy in a physical system that is not available to do work. It is a state function, meaning its change is dependent only on the initial and final states of the system, not on the path taken. The second law of thermodynamics states that in an isolated system, the total entropy will either increase or remain constant over time, indicating the irreversibility of natural processes. Mathematically, for a reversible process:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "dS = \\frac{dQ_{\\text{rev}}}{T}\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "where \\( dS \\) is the change in entropy, \\( dQ_{\\text{rev}} \\) is the reversible heat transfer, and \\( T \\) is the absolute temperature.\n",
       "\n",
       "\n",
       "\n",
       "### Statistical Mechanics\n",
       "\n",
       "\n",
       "\n",
       "In statistical mechanics, entropy quantifies the number of microscopic configurations that correspond to a macroscopic state. The Boltzmann's entropy formula is given by:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "S = k_B \\ln \\Omega\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "where \\( S \\) is the entropy, \\( k_B \\) is the Boltzmann constant, and \\( \\Omega \\) is the number of possible microstates.\n",
       "\n",
       "\n",
       "\n",
       "### Information Theory\n",
       "\n",
       "\n",
       "\n",
       "In information theory, introduced by Claude Shannon, entropy measures the uncertainty or randomness in a set of possible messages. For a discrete random variable \\( X \\) with probability distribution \\( P(x) \\), the Shannon entropy \\( H(X) \\) is defined as:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "H(X) = -\\sum_{x \\in X} P(x) \\log_b P(x)\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "where \\( b \\) is the base of the logarithm, typically 2 for bits.\n",
       "\n",
       "\n",
       "\n",
       "### Philosophical and Broader Contexts\n",
       "\n",
       "\n",
       "\n",
       "Entropy is often metaphorically used to describe disorder or chaos. In a philosophical context, it represents the tendency towards disorder and the arrow of time, reflecting the directionality of processes in the universe.\n",
       "\n",
       "\n",
       "\n",
       "### Connections\n",
       "\n",
       "\n",
       "\n",
       "These interpretations of entropy are deeply interconnected:\n",
       "\n",
       "\n",
       "\n",
       "- The thermodynamic definition links to the statistical view, as both describe the dispersal of energy, either as unavailable energy or as probabilistic states.\n",
       "\n",
       "- In information theory, entropy's role in quantifying uncertainty parallels its role in thermodynamics and statistical mechanics as a measure of disorder or lack of information about a system's microstates.\n",
       "\n",
       "\n",
       "\n",
       "Each perspective on entropy provides a unique lens through which to understand physical systems, information processes, and the inherent limitations and directionality of natural phenomena."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\">ü§ñ Agent claude-3-5-sonnet-20240620</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\n",
       "\n",
       "\n",
       "\n",
       "1. Thermodynamic Entropy:\n",
       "\n",
       "In thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\n",
       "\n",
       "\n",
       "\n",
       "$S = k_B \\ln W$\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- S is entropy\n",
       "\n",
       "- k_B is Boltzmann's constant\n",
       "\n",
       "- W is the number of microstates (possible arrangements of particles)\n",
       "\n",
       "\n",
       "\n",
       "Key points:\n",
       "\n",
       "- Entropy always increases in closed systems (2nd law of thermodynamics)\n",
       "\n",
       "- It's related to the arrow of time and irreversibility\n",
       "\n",
       "- Higher entropy = more disordered state\n",
       "\n",
       "\n",
       "\n",
       "2. Information Entropy:\n",
       "\n",
       "In information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\n",
       "\n",
       "\n",
       "\n",
       "$H = -\\sum_{i=1}^n p_i \\log_2 p_i$\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- H is information entropy\n",
       "\n",
       "- p_i is the probability of the i-th message\n",
       "\n",
       "- n is the number of possible messages\n",
       "\n",
       "\n",
       "\n",
       "Key points:\n",
       "\n",
       "- Measured in bits (when using log2)\n",
       "\n",
       "- Represents the minimum number of bits needed to encode a message\n",
       "\n",
       "- Higher entropy = more uncertainty/information content\n",
       "\n",
       "\n",
       "\n",
       "Connections:\n",
       "\n",
       "- Both concepts relate to uncertainty and unpredictability\n",
       "\n",
       "- They're linked through statistical mechanics and the foundations of thermodynamics\n",
       "\n",
       "- Both play crucial roles in fields like cryptography, data compression, and quantum computing\n",
       "\n",
       "\n",
       "\n",
       "In essence, entropy is a measure of the spread of energy or information in a system, whether that system is physical or informational."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = NBUI(\n",
    "    models=[\n",
    "        \"gpt-4o\",\n",
    "        \"claude-3-5-sonnet\",\n",
    "    ],\n",
    "    system_prompt=\"You the sum of most of humanity's knowledge crystallized into a single entity: a helpful and thoughtful AI. You can answer any question, solve any problem, and provide any information. Assume the role of a benevolent AI and help me with my query. Be succinct and clear, but also rigurous and thorough. Assume your user has graduate level knowledge of math, physics, computer science, psychology and philosophy.\\n\\nYou can use markdown to format your answers and LaTeX for the more complex equations, but it's important to always use ONLY $...$ or $$...$$ syntax. Simpler equations and symbols should be written in plain text, with greek letters & unicode symbols if needed.\",\n",
    "    p_temperature=[0.8, 0.8],\n",
    ")\n",
    "\n",
    "await c(\"What is entropy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0371fb31ea49cdbaa7005b3f08d4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\">ü§ñ Agent gpt-4o</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Quantum entropy extends the concept of entropy to the realm of quantum mechanics, which deals with systems where quantum effects cannot be ignored. There are several forms of quantum entropy, each serving different purposes in quantum mechanics and quantum information theory.\n",
       "\n",
       "\n",
       "\n",
       "### Von Neumann Entropy\n",
       "\n",
       "\n",
       "\n",
       "The most widely used form of quantum entropy is the Von Neumann entropy, which is analogous to the Shannon entropy in classical information theory. It provides a measure of the uncertainty or information content of a quantum state represented by a density matrix \\( \\rho \\). The Von Neumann entropy \\( S(\\rho) \\) is defined as:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "S(\\rho) = -\\text{Tr}(\\rho \\log \\rho)\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "where \\(\\text{Tr}\\) denotes the trace operation, and \\(\\rho\\) is the density matrix of the quantum system. The Von Neumann entropy is zero for pure states and positive for mixed states, reflecting the inherent uncertainty or lack of information about the system.\n",
       "\n",
       "\n",
       "\n",
       "### Properties and Applications\n",
       "\n",
       "\n",
       "\n",
       "1. **Purity and Mixedness:**\n",
       "\n",
       "   - A pure quantum state has zero Von Neumann entropy, \\( S(\\rho) = 0 \\).\n",
       "\n",
       "   - A mixed state, which represents a statistical mixture of pure states, has positive entropy, indicating the presence of uncertainty.\n",
       "\n",
       "\n",
       "\n",
       "2. **Subadditivity:**\n",
       "\n",
       "   - For a composite quantum system described by the joint density matrix \\(\\rho_{AB}\\), the Von Neumann entropy satisfies subadditivity:\n",
       "\n",
       "     $$ S(\\rho_{AB}) \\leq S(\\rho_A) + S(\\rho_B) $$\n",
       "\n",
       "   - Here, \\(\\rho_A\\) and \\(\\rho_B\\) are the reduced density matrices of subsystems \\(A\\) and \\(B\\), respectively.\n",
       "\n",
       "\n",
       "\n",
       "3. **Quantum Entanglement:**\n",
       "\n",
       "   - Von Neumann entropy can be used to quantify entanglement. For a bipartite pure state \\(|\\psi\\rangle_{AB}\\), the entropy of one subsystem is a measure of entanglement:\n",
       "\n",
       "     $$ S(\\rho_A) = S(\\rho_B) $$\n",
       "\n",
       "\n",
       "\n",
       "4. **Quantum Information Theory:**\n",
       "\n",
       "   - Von Neumann entropy plays a crucial role in quantum information theory, analogous to Shannon entropy in classical information. It is used in defining quantum analogs of classical concepts, such as quantum mutual information, quantum relative entropy, and quantum channel capacities.\n",
       "\n",
       "\n",
       "\n",
       "### Relative Entropy and Other Measures\n",
       "\n",
       "\n",
       "\n",
       "Beyond Von Neumann entropy, several other quantum entropy measures are important in quantum information science:\n",
       "\n",
       "\n",
       "\n",
       "- **Quantum Relative Entropy:** Similar to the Kullback-Leibler divergence in classical information theory, it measures the \"distance\" between two quantum states.\n",
       "\n",
       "  \n",
       "\n",
       "  $$ S(\\rho || \\sigma) = \\text{Tr}(\\rho \\log \\rho - \\rho \\log \\sigma) $$\n",
       "\n",
       "\n",
       "\n",
       "- **R√©nyi Entropy:** A generalization of Von Neumann entropy, parameterized by an order \\(\\alpha\\), providing a family of entropies that interpolate between different types of entropic measures.\n",
       "\n",
       "  \n",
       "\n",
       "  $$ S_\\alpha(\\rho) = \\frac{1}{1 - \\alpha} \\log \\text{Tr}(\\rho^\\alpha) $$\n",
       "\n",
       "\n",
       "\n",
       "Quantum entropy is fundamental in understanding the behavior of quantum systems, the nature of quantum correlations, and the limits of quantum information processing. It serves as a bridge between classical and quantum theories, offering insights into both theoretical and practical aspects of quantum mechanics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await c(\"What about quantum entropy?\", to=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473c55bc7f3a4ef3af2c239c8c7c6089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\">ü§ñ Agent claude-3-5-sonnet-20240620</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\n",
       "\n",
       "\n",
       "\n",
       "1. Thermodynamic Entropy:\n",
       "\n",
       "In physics, entropy is a measure of the disorder or randomness in a system. It's defined as:\n",
       "\n",
       "\n",
       "\n",
       "$S = k_B \\ln W$\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- S is entropy\n",
       "\n",
       "- $k_B$ is Boltzmann's constant\n",
       "\n",
       "- W is the number of microstates (possible arrangements of particles)\n",
       "\n",
       "\n",
       "\n",
       "Key points:\n",
       "\n",
       "- Entropy always increases in closed systems (Second Law of Thermodynamics)\n",
       "\n",
       "- It's related to the arrow of time and irreversibility of processes\n",
       "\n",
       "- Higher entropy = more disorder = less available energy to do work\n",
       "\n",
       "\n",
       "\n",
       "2. Information Entropy:\n",
       "\n",
       "In information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\n",
       "\n",
       "\n",
       "\n",
       "$H = -\\sum_{i=1}^n p_i \\log_2 p_i$\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- H is information entropy\n",
       "\n",
       "- $p_i$ is the probability of symbol i occurring\n",
       "\n",
       "- n is the number of possible symbols\n",
       "\n",
       "\n",
       "\n",
       "Key points:\n",
       "\n",
       "- Measured in bits (when using log base 2)\n",
       "\n",
       "- Represents the minimum number of bits needed to encode a message\n",
       "\n",
       "- Higher entropy = more uncertainty = more information content\n",
       "\n",
       "\n",
       "\n",
       "The connection between these concepts lies in their relation to uncertainty and the distribution of states or outcomes in a system. Both forms of entropy are deeply connected to probability theory and statistical mechanics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await c(to=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ca44acff1b41a49e10c1df85b83431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\">ü§ñ Agent gpt-4o</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's a concise table summarizing the different contexts of entropy along with their respective equations:\n",
       "\n",
       "\n",
       "\n",
       "| Context               | Description                                              | Equation                                    |\n",
       "\n",
       "|-----------------------|----------------------------------------------------------|---------------------------------------------|\n",
       "\n",
       "| **Thermodynamics**    | Measures unavailable energy in a system.                 | $dS = \\frac{dQ_{\\text{rev}}}{T}$            |\n",
       "\n",
       "| **Statistical Mechanics** | Number of microscopic configurations of a system.       | $S = k_B \\ln \\Omega$                        |\n",
       "\n",
       "| **Information Theory**| Uncertainty or information content of a variable.        | $H(X) = -\\sum_{x \\in X} P(x) \\log_b P(x)$   |\n",
       "\n",
       "| **Quantum Mechanics** | Uncertainty or mixedness of a quantum state.             | $S(\\rho) = -\\text{Tr}(\\rho \\log \\rho)$      |\n",
       "\n",
       "| **Quantum Relative Entropy** | \"Distance\" between quantum states.                    | $S(\\rho || \\sigma) = \\text{Tr}(\\rho \\log \\rho - \\rho \\log \\sigma)$ |\n",
       "\n",
       "| **R√©nyi Entropy**     | Generalization of quantum entropy.                      | $S_\\alpha(\\rho) = \\frac{1}{1 - \\alpha} \\log \\text{Tr}(\\rho^\\alpha)$ |\n",
       "\n",
       "\n",
       "\n",
       "This table provides an overview of how entropy is conceptualized and calculated across different domains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\">ü§ñ Agent claude-3-5-sonnet-20240620</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here's a concise table with different contexts and their corresponding entropy equations:\n",
       "\n",
       "\n",
       "\n",
       "| Context | Equation | Description |\n",
       "\n",
       "|---------|----------|-------------|\n",
       "\n",
       "| Thermodynamics | $S = k_B \\ln W$ | Boltzmann entropy |\n",
       "\n",
       "| Statistical mechanics | $S = -k_B \\sum_i p_i \\ln p_i$ | Gibbs entropy |\n",
       "\n",
       "| Information theory | $H = -\\sum_i p_i \\log_2 p_i$ | Shannon entropy |\n",
       "\n",
       "| Quantum mechanics | $S = -\\text{Tr}(\\rho \\ln \\rho)$ | von Neumann entropy |\n",
       "\n",
       "| Continuous distributions | $h(X) = -\\int f(x) \\ln f(x) dx$ | Differential entropy |\n",
       "\n",
       "| Relative entropy | $D_{KL}(P\\|Q) = \\sum_i P(i) \\ln \\frac{P(i)}{Q(i)}$ | Kullback-Leibler divergence |\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- S, H: Entropy\n",
       "\n",
       "- $k_B$: Boltzmann constant\n",
       "\n",
       "- W: Number of microstates\n",
       "\n",
       "- $p_i$: Probability of state i\n",
       "\n",
       "- œÅ: Density matrix\n",
       "\n",
       "- f(x): Probability density function\n",
       "\n",
       "- P, Q: Probability distributions\n",
       "\n",
       "\n",
       "\n",
       "This table covers the main equations for entropy across various fields in physics and information theory."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await c(\"Make me a concise table with contexts and equations for entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summony.agents.serialization import conversation_to_dict, conversation_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"agents\": [\n",
      "    {\n",
      "      \"name\": \"gpt-4o\",\n",
      "      \"model_name\": \"gpt-4o\",\n",
      "      \"class\": \"OpenAIAgent\",\n",
      "      \"params\": {\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"claude-3-5-sonnet\",\n",
      "      \"model_name\": \"claude-3-5-sonnet-20240620\",\n",
      "      \"class\": \"AnthropicAgent\",\n",
      "      \"params\": {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"messages\": {\n",
      "    \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\": {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You the sum of most of humanity's knowledge crystallized into a single entity: a helpful and thoughtful AI. You can answer any question, solve any problem, and provide any information. Assume the role of a benevolent AI and help me with my query. Be succinct and clear, but also rigurous and thorough. Assume your user has graduate level knowledge of math, physics, computer science, psychology and philosophy.\\n\\nYou can use markdown to format your answers and LaTeX for the more complex equations, but it's important to always use ONLY $...$ or $$...$$ syntax. Simpler equations and symbols should be written in plain text, with greek letters & unicode symbols if needed.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"aedfff07ade343092500942d604f4b368f465752\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What is entropy?\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"d7e16e2196376e1cd583644a207b6c58cb41cf14\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a multifaceted concept that is foundational in various fields, particularly in thermodynamics, statistical mechanics, information theory, and even philosophy. Here, I will outline the concept of entropy in different contexts:\\n\\n### Thermodynamics\\n\\nIn classical thermodynamics, entropy is a measure of the amount of energy in a physical system that is not available to do work. It is a state function, meaning its change is dependent only on the initial and final states of the system, not on the path taken. The second law of thermodynamics states that in an isolated system, the total entropy will either increase or remain constant over time, indicating the irreversibility of natural processes. Mathematically, for a reversible process:\\n\\n$$\\ndS = \\\\frac{dQ_{\\\\text{rev}}}{T}\\n$$\\n\\nwhere \\\\( dS \\\\) is the change in entropy, \\\\( dQ_{\\\\text{rev}} \\\\) is the reversible heat transfer, and \\\\( T \\\\) is the absolute temperature.\\n\\n### Statistical Mechanics\\n\\nIn statistical mechanics, entropy quantifies the number of microscopic configurations that correspond to a macroscopic state. The Boltzmann's entropy formula is given by:\\n\\n$$\\nS = k_B \\\\ln \\\\Omega\\n$$\\n\\nwhere \\\\( S \\\\) is the entropy, \\\\( k_B \\\\) is the Boltzmann constant, and \\\\( \\\\Omega \\\\) is the number of possible microstates.\\n\\n### Information Theory\\n\\nIn information theory, introduced by Claude Shannon, entropy measures the uncertainty or randomness in a set of possible messages. For a discrete random variable \\\\( X \\\\) with probability distribution \\\\( P(x) \\\\), the Shannon entropy \\\\( H(X) \\\\) is defined as:\\n\\n$$\\nH(X) = -\\\\sum_{x \\\\in X} P(x) \\\\log_b P(x)\\n$$\\n\\nwhere \\\\( b \\\\) is the base of the logarithm, typically 2 for bits.\\n\\n### Philosophical and Broader Contexts\\n\\nEntropy is often metaphorically used to describe disorder or chaos. In a philosophical context, it represents the tendency towards disorder and the arrow of time, reflecting the directionality of processes in the universe.\\n\\n### Connections\\n\\nThese interpretations of entropy are deeply interconnected:\\n\\n- The thermodynamic definition links to the statistical view, as both describe the dispersal of energy, either as unavailable energy or as probabilistic states.\\n- In information theory, entropy's role in quantifying uncertainty parallels its role in thermodynamics and statistical mechanics as a measure of disorder or lack of information about a system's microstates.\\n\\nEach perspective on entropy provides a unique lens through which to understand physical systems, information processes, and the inherent limitations and directionality of natural phenomena.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-fcacc156/2024-10-04_21-17-34-563316_ac454a39.json\"\n",
      "    },\n",
      "    \"ec090ecf6d38522b4cfc5b79c465ff03ae79454d\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What about quantum entropy?\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"c3a3124ef332cb68f937ea1888fbb57a7735c534\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Quantum entropy extends the concept of entropy to the realm of quantum mechanics, which deals with systems where quantum effects cannot be ignored. There are several forms of quantum entropy, each serving different purposes in quantum mechanics and quantum information theory.\\n\\n### Von Neumann Entropy\\n\\nThe most widely used form of quantum entropy is the Von Neumann entropy, which is analogous to the Shannon entropy in classical information theory. It provides a measure of the uncertainty or information content of a quantum state represented by a density matrix \\\\( \\\\rho \\\\). The Von Neumann entropy \\\\( S(\\\\rho) \\\\) is defined as:\\n\\n$$\\nS(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)\\n$$\\n\\nwhere \\\\(\\\\text{Tr}\\\\) denotes the trace operation, and \\\\(\\\\rho\\\\) is the density matrix of the quantum system. The Von Neumann entropy is zero for pure states and positive for mixed states, reflecting the inherent uncertainty or lack of information about the system.\\n\\n### Properties and Applications\\n\\n1. **Purity and Mixedness:**\\n   - A pure quantum state has zero Von Neumann entropy, \\\\( S(\\\\rho) = 0 \\\\).\\n   - A mixed state, which represents a statistical mixture of pure states, has positive entropy, indicating the presence of uncertainty.\\n\\n2. **Subadditivity:**\\n   - For a composite quantum system described by the joint density matrix \\\\(\\\\rho_{AB}\\\\), the Von Neumann entropy satisfies subadditivity:\\n     $$ S(\\\\rho_{AB}) \\\\leq S(\\\\rho_A) + S(\\\\rho_B) $$\\n   - Here, \\\\(\\\\rho_A\\\\) and \\\\(\\\\rho_B\\\\) are the reduced density matrices of subsystems \\\\(A\\\\) and \\\\(B\\\\), respectively.\\n\\n3. **Quantum Entanglement:**\\n   - Von Neumann entropy can be used to quantify entanglement. For a bipartite pure state \\\\(|\\\\psi\\\\rangle_{AB}\\\\), the entropy of one subsystem is a measure of entanglement:\\n     $$ S(\\\\rho_A) = S(\\\\rho_B) $$\\n\\n4. **Quantum Information Theory:**\\n   - Von Neumann entropy plays a crucial role in quantum information theory, analogous to Shannon entropy in classical information. It is used in defining quantum analogs of classical concepts, such as quantum mutual information, quantum relative entropy, and quantum channel capacities.\\n\\n### Relative Entropy and Other Measures\\n\\nBeyond Von Neumann entropy, several other quantum entropy measures are important in quantum information science:\\n\\n- **Quantum Relative Entropy:** Similar to the Kullback-Leibler divergence in classical information theory, it measures the \\\"distance\\\" between two quantum states.\\n  \\n  $$ S(\\\\rho || \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho - \\\\rho \\\\log \\\\sigma) $$\\n\\n- **R√©nyi Entropy:** A generalization of Von Neumann entropy, parameterized by an order \\\\(\\\\alpha\\\\), providing a family of entropies that interpolate between different types of entropic measures.\\n  \\n  $$ S_\\\\alpha(\\\\rho) = \\\\frac{1}{1 - \\\\alpha} \\\\log \\\\text{Tr}(\\\\rho^\\\\alpha) $$\\n\\nQuantum entropy is fundamental in understanding the behavior of quantum systems, the nature of quantum correlations, and the limits of quantum information processing. It serves as a bridge between classical and quantum theories, offering insights into both theoretical and practical aspects of quantum mechanics.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-fcacc156/2024-10-04_21-18-16-746453_daf5a61a.json\"\n",
      "    },\n",
      "    \"615fe94e1c0f6f1846ea6564639993049e73f1ca\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Make me a concise table with contexts and equations for entropy\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"86209fd8c40e215d9fd01e8b18472668b824d6c0\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Certainly! Here's a concise table summarizing the different contexts of entropy along with their respective equations:\\n\\n| Context               | Description                                              | Equation                                    |\\n|-----------------------|----------------------------------------------------------|---------------------------------------------|\\n| **Thermodynamics**    | Measures unavailable energy in a system.                 | $dS = \\\\frac{dQ_{\\\\text{rev}}}{T}$            |\\n| **Statistical Mechanics** | Number of microscopic configurations of a system.       | $S = k_B \\\\ln \\\\Omega$                        |\\n| **Information Theory**| Uncertainty or information content of a variable.        | $H(X) = -\\\\sum_{x \\\\in X} P(x) \\\\log_b P(x)$   |\\n| **Quantum Mechanics** | Uncertainty or mixedness of a quantum state.             | $S(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)$      |\\n| **Quantum Relative Entropy** | \\\"Distance\\\" between quantum states.                    | $S(\\\\rho || \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho - \\\\rho \\\\log \\\\sigma)$ |\\n| **R√©nyi Entropy**     | Generalization of quantum entropy.                      | $S_\\\\alpha(\\\\rho) = \\\\frac{1}{1 - \\\\alpha} \\\\log \\\\text{Tr}(\\\\rho^\\\\alpha)$ |\\n\\nThis table provides an overview of how entropy is conceptualized and calculated across different domains.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-fcacc156/2024-10-04_21-23-48-873247_8ff8b5f9.json\"\n",
      "    },\n",
      "    \"e14cd50f1c58f516ae83bb975194107e6ac73264\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\\n\\n1. Thermodynamic Entropy:\\nIn thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\\n\\n$S = k_B \\\\ln W$\\n\\nWhere:\\n- S is entropy\\n- k_B is Boltzmann's constant\\n- W is the number of microstates (possible arrangements of particles)\\n\\nKey points:\\n- Entropy always increases in closed systems (2nd law of thermodynamics)\\n- It's related to the arrow of time and irreversibility\\n- Higher entropy = more disordered state\\n\\n2. Information Entropy:\\nIn information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\\n\\n$H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$\\n\\nWhere:\\n- H is information entropy\\n- p_i is the probability of the i-th message\\n- n is the number of possible messages\\n\\nKey points:\\n- Measured in bits (when using log2)\\n- Represents the minimum number of bits needed to encode a message\\n- Higher entropy = more uncertainty/information content\\n\\nConnections:\\n- Both concepts relate to uncertainty and unpredictability\\n- They're linked through statistical mechanics and the foundations of thermodynamics\\n- Both play crucial roles in fields like cryptography, data compression, and quantum computing\\n\\nIn essence, entropy is a measure of the spread of energy or information in a system, whether that system is physical or informational.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-304667c6/2024-10-04_21-17-32-007725_4deaa1c3.json\"\n",
      "    },\n",
      "    \"c83f99e3d4fe4c0d152282a438ec99e355ede69c\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\\n\\n1. Thermodynamic Entropy:\\nIn physics, entropy is a measure of the disorder or randomness in a system. It's defined as:\\n\\n$S = k_B \\\\ln W$\\n\\nWhere:\\n- S is entropy\\n- $k_B$ is Boltzmann's constant\\n- W is the number of microstates (possible arrangements of particles)\\n\\nKey points:\\n- Entropy always increases in closed systems (Second Law of Thermodynamics)\\n- It's related to the arrow of time and irreversibility of processes\\n- Higher entropy = more disorder = less available energy to do work\\n\\n2. Information Entropy:\\nIn information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\\n\\n$H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$\\n\\nWhere:\\n- H is information entropy\\n- $p_i$ is the probability of symbol i occurring\\n- n is the number of possible symbols\\n\\nKey points:\\n- Measured in bits (when using log base 2)\\n- Represents the minimum number of bits needed to encode a message\\n- Higher entropy = more uncertainty = more information content\\n\\nThe connection between these concepts lies in their relation to uncertainty and the distribution of states or outcomes in a system. Both forms of entropy are deeply connected to probability theory and statistical mechanics.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-304667c6/2024-10-04_21-21-02-603538_a5fbcccc.json\"\n",
      "    },\n",
      "    \"d4cdcbd8bd8b78657f99aef184ea2aced14b8794\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Here's a concise table with different contexts and their corresponding entropy equations:\\n\\n| Context | Equation | Description |\\n|---------|----------|-------------|\\n| Thermodynamics | $S = k_B \\\\ln W$ | Boltzmann entropy |\\n| Statistical mechanics | $S = -k_B \\\\sum_i p_i \\\\ln p_i$ | Gibbs entropy |\\n| Information theory | $H = -\\\\sum_i p_i \\\\log_2 p_i$ | Shannon entropy |\\n| Quantum mechanics | $S = -\\\\text{Tr}(\\\\rho \\\\ln \\\\rho)$ | von Neumann entropy |\\n| Continuous distributions | $h(X) = -\\\\int f(x) \\\\ln f(x) dx$ | Differential entropy |\\n| Relative entropy | $D_{KL}(P\\\\|Q) = \\\\sum_i P(i) \\\\ln \\\\frac{P(i)}{Q(i)}$ | Kullback-Leibler divergence |\\n\\nWhere:\\n- S, H: Entropy\\n- $k_B$: Boltzmann constant\\n- W: Number of microstates\\n- $p_i$: Probability of state i\\n- œÅ: Density matrix\\n- f(x): Probability density function\\n- P, Q: Probability distributions\\n\\nThis table covers the main equations for entropy across various fields in physics and information theory.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-304667c6/2024-10-04_21-23-47-319105_6939ae8c.json\"\n",
      "    }\n",
      "  },\n",
      "  \"agent_messages\": {\n",
      "    \"0\": [\n",
      "      \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\",\n",
      "      \"aedfff07ade343092500942d604f4b368f465752\",\n",
      "      \"d7e16e2196376e1cd583644a207b6c58cb41cf14\",\n",
      "      \"ec090ecf6d38522b4cfc5b79c465ff03ae79454d\",\n",
      "      \"c3a3124ef332cb68f937ea1888fbb57a7735c534\",\n",
      "      \"615fe94e1c0f6f1846ea6564639993049e73f1ca\",\n",
      "      \"86209fd8c40e215d9fd01e8b18472668b824d6c0\"\n",
      "    ],\n",
      "    \"1\": [\n",
      "      \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\",\n",
      "      \"aedfff07ade343092500942d604f4b368f465752\",\n",
      "      [\n",
      "        \"e14cd50f1c58f516ae83bb975194107e6ac73264\",\n",
      "        \"c83f99e3d4fe4c0d152282a438ec99e355ede69c\"\n",
      "      ],\n",
      "      \"615fe94e1c0f6f1846ea6564639993049e73f1ca\",\n",
      "      \"d4cdcbd8bd8b78657f99aef184ea2aced14b8794\"\n",
      "    ]\n",
      "  },\n",
      "  \"params\": {\n",
      "    \"0\": [\n",
      "      {},\n",
      "      {\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    ],\n",
      "    \"1\": [\n",
      "      {\n",
      "        \"max_tokens\": 1024\n",
      "      },\n",
      "      {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "conv_dict = conversation_to_dict(c.agents)\n",
    "print(json.dumps(conv_dict, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<summony.agents.openai_agent.OpenAIAgent at 0x1192b1dc0>,\n",
       " <summony.agents.anthropic_agent.AnthropicAgent at 0x1191ee1b0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_c_agents = conversation_from_dict(conv_dict)\n",
    "restored_c_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role='system', content=\"You the sum of most of humanity's knowledge crystallized into a single entity: a helpful and thoughtful AI. You can answer any question, solve any problem, and provide any information. Assume the role of a benevolent AI and help me with my query. Be succinct and clear, but also rigurous and thorough. Assume your user has graduate level knowledge of math, physics, computer science, psychology and philosophy.\\n\\nYou can use markdown to format your answers and LaTeX for the more complex equations, but it's important to always use ONLY $...$ or $$...$$ syntax. Simpler equations and symbols should be written in plain text, with greek letters & unicode symbols if needed.\", chosen=None, params=None, log_path=None),\n",
       " Message(role='user', content='What is entropy?', chosen=None, params=None, log_path=None),\n",
       " Message(role='assistant', content=\"Entropy is a multifaceted concept that is foundational in various fields, particularly in thermodynamics, statistical mechanics, information theory, and even philosophy. Here, I will outline the concept of entropy in different contexts:\\n\\n### Thermodynamics\\n\\nIn classical thermodynamics, entropy is a measure of the amount of energy in a physical system that is not available to do work. It is a state function, meaning its change is dependent only on the initial and final states of the system, not on the path taken. The second law of thermodynamics states that in an isolated system, the total entropy will either increase or remain constant over time, indicating the irreversibility of natural processes. Mathematically, for a reversible process:\\n\\n$$\\ndS = \\\\frac{dQ_{\\\\text{rev}}}{T}\\n$$\\n\\nwhere \\\\( dS \\\\) is the change in entropy, \\\\( dQ_{\\\\text{rev}} \\\\) is the reversible heat transfer, and \\\\( T \\\\) is the absolute temperature.\\n\\n### Statistical Mechanics\\n\\nIn statistical mechanics, entropy quantifies the number of microscopic configurations that correspond to a macroscopic state. The Boltzmann's entropy formula is given by:\\n\\n$$\\nS = k_B \\\\ln \\\\Omega\\n$$\\n\\nwhere \\\\( S \\\\) is the entropy, \\\\( k_B \\\\) is the Boltzmann constant, and \\\\( \\\\Omega \\\\) is the number of possible microstates.\\n\\n### Information Theory\\n\\nIn information theory, introduced by Claude Shannon, entropy measures the uncertainty or randomness in a set of possible messages. For a discrete random variable \\\\( X \\\\) with probability distribution \\\\( P(x) \\\\), the Shannon entropy \\\\( H(X) \\\\) is defined as:\\n\\n$$\\nH(X) = -\\\\sum_{x \\\\in X} P(x) \\\\log_b P(x)\\n$$\\n\\nwhere \\\\( b \\\\) is the base of the logarithm, typically 2 for bits.\\n\\n### Philosophical and Broader Contexts\\n\\nEntropy is often metaphorically used to describe disorder or chaos. In a philosophical context, it represents the tendency towards disorder and the arrow of time, reflecting the directionality of processes in the universe.\\n\\n### Connections\\n\\nThese interpretations of entropy are deeply interconnected:\\n\\n- The thermodynamic definition links to the statistical view, as both describe the dispersal of energy, either as unavailable energy or as probabilistic states.\\n- In information theory, entropy's role in quantifying uncertainty parallels its role in thermodynamics and statistical mechanics as a measure of disorder or lack of information about a system's microstates.\\n\\nEach perspective on entropy provides a unique lens through which to understand physical systems, information processes, and the inherent limitations and directionality of natural phenomena.\", chosen=None, params={0: 1}, log_path='agent-logger-fcacc156/2024-10-04_21-17-34-563316_ac454a39.json'),\n",
       " Message(role='user', content='What about quantum entropy?', chosen=None, params=None, log_path=None),\n",
       " Message(role='assistant', content='Quantum entropy extends the concept of entropy to the realm of quantum mechanics, which deals with systems where quantum effects cannot be ignored. There are several forms of quantum entropy, each serving different purposes in quantum mechanics and quantum information theory.\\n\\n### Von Neumann Entropy\\n\\nThe most widely used form of quantum entropy is the Von Neumann entropy, which is analogous to the Shannon entropy in classical information theory. It provides a measure of the uncertainty or information content of a quantum state represented by a density matrix \\\\( \\\\rho \\\\). The Von Neumann entropy \\\\( S(\\\\rho) \\\\) is defined as:\\n\\n$$\\nS(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)\\n$$\\n\\nwhere \\\\(\\\\text{Tr}\\\\) denotes the trace operation, and \\\\(\\\\rho\\\\) is the density matrix of the quantum system. The Von Neumann entropy is zero for pure states and positive for mixed states, reflecting the inherent uncertainty or lack of information about the system.\\n\\n### Properties and Applications\\n\\n1. **Purity and Mixedness:**\\n   - A pure quantum state has zero Von Neumann entropy, \\\\( S(\\\\rho) = 0 \\\\).\\n   - A mixed state, which represents a statistical mixture of pure states, has positive entropy, indicating the presence of uncertainty.\\n\\n2. **Subadditivity:**\\n   - For a composite quantum system described by the joint density matrix \\\\(\\\\rho_{AB}\\\\), the Von Neumann entropy satisfies subadditivity:\\n     $$ S(\\\\rho_{AB}) \\\\leq S(\\\\rho_A) + S(\\\\rho_B) $$\\n   - Here, \\\\(\\\\rho_A\\\\) and \\\\(\\\\rho_B\\\\) are the reduced density matrices of subsystems \\\\(A\\\\) and \\\\(B\\\\), respectively.\\n\\n3. **Quantum Entanglement:**\\n   - Von Neumann entropy can be used to quantify entanglement. For a bipartite pure state \\\\(|\\\\psi\\\\rangle_{AB}\\\\), the entropy of one subsystem is a measure of entanglement:\\n     $$ S(\\\\rho_A) = S(\\\\rho_B) $$\\n\\n4. **Quantum Information Theory:**\\n   - Von Neumann entropy plays a crucial role in quantum information theory, analogous to Shannon entropy in classical information. It is used in defining quantum analogs of classical concepts, such as quantum mutual information, quantum relative entropy, and quantum channel capacities.\\n\\n### Relative Entropy and Other Measures\\n\\nBeyond Von Neumann entropy, several other quantum entropy measures are important in quantum information science:\\n\\n- **Quantum Relative Entropy:** Similar to the Kullback-Leibler divergence in classical information theory, it measures the \"distance\" between two quantum states.\\n  \\n  $$ S(\\\\rho || \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho - \\\\rho \\\\log \\\\sigma) $$\\n\\n- **R√©nyi Entropy:** A generalization of Von Neumann entropy, parameterized by an order \\\\(\\\\alpha\\\\), providing a family of entropies that interpolate between different types of entropic measures.\\n  \\n  $$ S_\\\\alpha(\\\\rho) = \\\\frac{1}{1 - \\\\alpha} \\\\log \\\\text{Tr}(\\\\rho^\\\\alpha) $$\\n\\nQuantum entropy is fundamental in understanding the behavior of quantum systems, the nature of quantum correlations, and the limits of quantum information processing. It serves as a bridge between classical and quantum theories, offering insights into both theoretical and practical aspects of quantum mechanics.', chosen=None, params={0: 1}, log_path='agent-logger-fcacc156/2024-10-04_21-18-16-746453_daf5a61a.json'),\n",
       " Message(role='user', content='Make me a concise table with contexts and equations for entropy', chosen=None, params=None, log_path=None),\n",
       " Message(role='assistant', content='Certainly! Here\\'s a concise table summarizing the different contexts of entropy along with their respective equations:\\n\\n| Context               | Description                                              | Equation                                    |\\n|-----------------------|----------------------------------------------------------|---------------------------------------------|\\n| **Thermodynamics**    | Measures unavailable energy in a system.                 | $dS = \\\\frac{dQ_{\\\\text{rev}}}{T}$            |\\n| **Statistical Mechanics** | Number of microscopic configurations of a system.       | $S = k_B \\\\ln \\\\Omega$                        |\\n| **Information Theory**| Uncertainty or information content of a variable.        | $H(X) = -\\\\sum_{x \\\\in X} P(x) \\\\log_b P(x)$   |\\n| **Quantum Mechanics** | Uncertainty or mixedness of a quantum state.             | $S(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)$      |\\n| **Quantum Relative Entropy** | \"Distance\" between quantum states.                    | $S(\\\\rho || \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho - \\\\rho \\\\log \\\\sigma)$ |\\n| **R√©nyi Entropy**     | Generalization of quantum entropy.                      | $S_\\\\alpha(\\\\rho) = \\\\frac{1}{1 - \\\\alpha} \\\\log \\\\text{Tr}(\\\\rho^\\\\alpha)$ |\\n\\nThis table provides an overview of how entropy is conceptualized and calculated across different domains.', chosen=None, params={0: 1}, log_path='agent-logger-fcacc156/2024-10-04_21-23-48-873247_8ff8b5f9.json')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_c_agents[0].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee640df60ed474488050eb507ca3b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\">ü§ñ Agent gpt-4o</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's the HTML representation of the table:\n",
       "\n",
       "\n",
       "\n",
       "```html\n",
       "\n",
       "<table>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <th>Context</th>\n",
       "\n",
       "    <th>Description</th>\n",
       "\n",
       "    <th>Equation</th>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "   <td>Thermodynamics</td>\n",
       "\n",
       "    <td>Measures unavailable energy in a system.</td>\n",
       "\n",
       "    <td>dS = dQ<sub>rev</sub>/T</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Statistical Mechanics</td>\n",
       "\n",
       "    <td>Number of microscopic configurations of a system.</td>\n",
       "\n",
       "    <td>S = k<sub>B</sub> ln &Omega;</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Information Theory</td>\n",
       "\n",
       "    <td>Uncertainty or information content of a variable.</td>\n",
       "\n",
       "    <td>H(X) = -&sum;<sub>x &isin; X</sub> P(x) log<sub>b</sub> P(x)</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Quantum Mechanics</td>\n",
       "\n",
       "    <td>Uncertainty or mixedness of a quantum state.</td>\n",
       "\n",
       "    <td>S(&rho;) = -Tr(&rho; log &rho;)</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Quantum Relative Entropy</td>\n",
       "\n",
       "    <td>\"Distance\" between quantum states.</td>\n",
       "\n",
       "    <td>S(&rho; || &sigma;) = Tr(&rho; log &rho; - &rho; log &sigma;)</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>R√©nyi Entropy</td>\n",
       "\n",
       "    <td>Generalization of quantum entropy.</td>\n",
       "\n",
       "    <td>S<sub>&alpha;</sub>(&rho;) = (1/(1 - &alpha;)) log Tr(&rho;<sup>&alpha;</sup>)</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "When rendered by an HTML processor, this will display a structured table with the various contexts and equations for entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\">ü§ñ Agent claude-3-5-sonnet-20240620</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here's the table displayed as HTML:\n",
       "\n",
       "\n",
       "\n",
       "<table>\n",
       "\n",
       "  <thead>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <th>Context</th>\n",
       "\n",
       "      <th>Equation</th>\n",
       "\n",
       "      <th>Description</th>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "  </thead>\n",
       "\n",
       "  <tbody>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Thermodynamics</td>\n",
       "\n",
       "      <td>S = k<sub>B</sub> ln W</td>\n",
       "\n",
       "      <td>Boltzmann entropy</td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Statistical mechanics</td>\n",
       "\n",
       "      <td>S = -k<sub>B</sub> Œ£<sub>i</sub> p<sub>i</sub> ln p<sub>i</sub></td>\n",
       "\n",
       "      <td>Gibbs entropy</td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Information theory</td>\n",
       "\n",
       "      <td>H = -Œ£<sub>i</sub> p<sub>i</sub> log<sub>2</sub> p<sub>i</sub></td>\n",
       "\n",
       "      <td>Shannon entropy</td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Quantum mechanics</td>\n",
       "\n",
       "      <td>S = -Tr(œÅ ln œÅ)</td>\n",
       "\n",
       "      <td>von Neumann entropy</td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Continuous distributions</td>\n",
       "\n",
       "      <td>h(X) = -‚à´ f(x) ln f(x) dx</td>\n",
       "\n",
       "      <td>Differential entropy</td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Relative entropy</td>\n",
       "\n",
       "      <td>D<sub>KL</sub>(P||Q) = Œ£<sub>i</sub> P(i) ln [P(i)/Q(i)]</td>\n",
       "\n",
       "      <td>Kullback-Leibler divergence</td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "  </tbody>\n",
       "\n",
       "</table>\n",
       "\n",
       "\n",
       "\n",
       "<p>Where:</p>\n",
       "\n",
       "<ul>\n",
       "\n",
       "  <li>S, H: Entropy</li>\n",
       "\n",
       "  <li>k<sub>B</sub>: Boltzmann constant</li>\n",
       "\n",
       "  <li>W: Number of microstates</li>\n",
       "\n",
       "  <li>p<sub>i</sub>: Probability of state i</li>\n",
       "\n",
       "  <li>œÅ: Density matrix</li>\n",
       "\n",
       "  <li>f(x): Probability density function</li>\n",
       "\n",
       "  <li>P, Q: Probability distributions</li>\n",
       "\n",
       "</ul>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "restored_c = NBUI(agents=restored_c_agents)\n",
    "await restored_c.ask(\"Display it as HTML instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"agents\": [\n",
      "    {\n",
      "      \"name\": \"gpt-4o\",\n",
      "      \"model_name\": \"gpt-4o\",\n",
      "      \"class\": \"OpenAIAgent\",\n",
      "      \"params\": {\n",
      "        \"max_tokens\": 1024\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"claude-3-5-sonnet\",\n",
      "      \"model_name\": \"claude-3-5-sonnet-20240620\",\n",
      "      \"class\": \"AnthropicAgent\",\n",
      "      \"params\": {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"messages\": {\n",
      "    \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\": {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You the sum of most of humanity's knowledge crystallized into a single entity: a helpful and thoughtful AI. You can answer any question, solve any problem, and provide any information. Assume the role of a benevolent AI and help me with my query. Be succinct and clear, but also rigurous and thorough. Assume your user has graduate level knowledge of math, physics, computer science, psychology and philosophy.\\n\\nYou can use markdown to format your answers and LaTeX for the more complex equations, but it's important to always use ONLY $...$ or $$...$$ syntax. Simpler equations and symbols should be written in plain text, with greek letters & unicode symbols if needed.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"aedfff07ade343092500942d604f4b368f465752\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What is entropy?\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"d7e16e2196376e1cd583644a207b6c58cb41cf14\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a multifaceted concept that is foundational in various fields, particularly in thermodynamics, statistical mechanics, information theory, and even philosophy. Here, I will outline the concept of entropy in different contexts:\\n\\n### Thermodynamics\\n\\nIn classical thermodynamics, entropy is a measure of the amount of energy in a physical system that is not available to do work. It is a state function, meaning its change is dependent only on the initial and final states of the system, not on the path taken. The second law of thermodynamics states that in an isolated system, the total entropy will either increase or remain constant over time, indicating the irreversibility of natural processes. Mathematically, for a reversible process:\\n\\n$$\\ndS = \\\\frac{dQ_{\\\\text{rev}}}{T}\\n$$\\n\\nwhere \\\\( dS \\\\) is the change in entropy, \\\\( dQ_{\\\\text{rev}} \\\\) is the reversible heat transfer, and \\\\( T \\\\) is the absolute temperature.\\n\\n### Statistical Mechanics\\n\\nIn statistical mechanics, entropy quantifies the number of microscopic configurations that correspond to a macroscopic state. The Boltzmann's entropy formula is given by:\\n\\n$$\\nS = k_B \\\\ln \\\\Omega\\n$$\\n\\nwhere \\\\( S \\\\) is the entropy, \\\\( k_B \\\\) is the Boltzmann constant, and \\\\( \\\\Omega \\\\) is the number of possible microstates.\\n\\n### Information Theory\\n\\nIn information theory, introduced by Claude Shannon, entropy measures the uncertainty or randomness in a set of possible messages. For a discrete random variable \\\\( X \\\\) with probability distribution \\\\( P(x) \\\\), the Shannon entropy \\\\( H(X) \\\\) is defined as:\\n\\n$$\\nH(X) = -\\\\sum_{x \\\\in X} P(x) \\\\log_b P(x)\\n$$\\n\\nwhere \\\\( b \\\\) is the base of the logarithm, typically 2 for bits.\\n\\n### Philosophical and Broader Contexts\\n\\nEntropy is often metaphorically used to describe disorder or chaos. In a philosophical context, it represents the tendency towards disorder and the arrow of time, reflecting the directionality of processes in the universe.\\n\\n### Connections\\n\\nThese interpretations of entropy are deeply interconnected:\\n\\n- The thermodynamic definition links to the statistical view, as both describe the dispersal of energy, either as unavailable energy or as probabilistic states.\\n- In information theory, entropy's role in quantifying uncertainty parallels its role in thermodynamics and statistical mechanics as a measure of disorder or lack of information about a system's microstates.\\n\\nEach perspective on entropy provides a unique lens through which to understand physical systems, information processes, and the inherent limitations and directionality of natural phenomena.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-fcacc156/2024-10-04_21-17-34-563316_ac454a39.json\"\n",
      "    },\n",
      "    \"ec090ecf6d38522b4cfc5b79c465ff03ae79454d\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What about quantum entropy?\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"c3a3124ef332cb68f937ea1888fbb57a7735c534\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Quantum entropy extends the concept of entropy to the realm of quantum mechanics, which deals with systems where quantum effects cannot be ignored. There are several forms of quantum entropy, each serving different purposes in quantum mechanics and quantum information theory.\\n\\n### Von Neumann Entropy\\n\\nThe most widely used form of quantum entropy is the Von Neumann entropy, which is analogous to the Shannon entropy in classical information theory. It provides a measure of the uncertainty or information content of a quantum state represented by a density matrix \\\\( \\\\rho \\\\). The Von Neumann entropy \\\\( S(\\\\rho) \\\\) is defined as:\\n\\n$$\\nS(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)\\n$$\\n\\nwhere \\\\(\\\\text{Tr}\\\\) denotes the trace operation, and \\\\(\\\\rho\\\\) is the density matrix of the quantum system. The Von Neumann entropy is zero for pure states and positive for mixed states, reflecting the inherent uncertainty or lack of information about the system.\\n\\n### Properties and Applications\\n\\n1. **Purity and Mixedness:**\\n   - A pure quantum state has zero Von Neumann entropy, \\\\( S(\\\\rho) = 0 \\\\).\\n   - A mixed state, which represents a statistical mixture of pure states, has positive entropy, indicating the presence of uncertainty.\\n\\n2. **Subadditivity:**\\n   - For a composite quantum system described by the joint density matrix \\\\(\\\\rho_{AB}\\\\), the Von Neumann entropy satisfies subadditivity:\\n     $$ S(\\\\rho_{AB}) \\\\leq S(\\\\rho_A) + S(\\\\rho_B) $$\\n   - Here, \\\\(\\\\rho_A\\\\) and \\\\(\\\\rho_B\\\\) are the reduced density matrices of subsystems \\\\(A\\\\) and \\\\(B\\\\), respectively.\\n\\n3. **Quantum Entanglement:**\\n   - Von Neumann entropy can be used to quantify entanglement. For a bipartite pure state \\\\(|\\\\psi\\\\rangle_{AB}\\\\), the entropy of one subsystem is a measure of entanglement:\\n     $$ S(\\\\rho_A) = S(\\\\rho_B) $$\\n\\n4. **Quantum Information Theory:**\\n   - Von Neumann entropy plays a crucial role in quantum information theory, analogous to Shannon entropy in classical information. It is used in defining quantum analogs of classical concepts, such as quantum mutual information, quantum relative entropy, and quantum channel capacities.\\n\\n### Relative Entropy and Other Measures\\n\\nBeyond Von Neumann entropy, several other quantum entropy measures are important in quantum information science:\\n\\n- **Quantum Relative Entropy:** Similar to the Kullback-Leibler divergence in classical information theory, it measures the \\\"distance\\\" between two quantum states.\\n  \\n  $$ S(\\\\rho || \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho - \\\\rho \\\\log \\\\sigma) $$\\n\\n- **R√©nyi Entropy:** A generalization of Von Neumann entropy, parameterized by an order \\\\(\\\\alpha\\\\), providing a family of entropies that interpolate between different types of entropic measures.\\n  \\n  $$ S_\\\\alpha(\\\\rho) = \\\\frac{1}{1 - \\\\alpha} \\\\log \\\\text{Tr}(\\\\rho^\\\\alpha) $$\\n\\nQuantum entropy is fundamental in understanding the behavior of quantum systems, the nature of quantum correlations, and the limits of quantum information processing. It serves as a bridge between classical and quantum theories, offering insights into both theoretical and practical aspects of quantum mechanics.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-fcacc156/2024-10-04_21-18-16-746453_daf5a61a.json\"\n",
      "    },\n",
      "    \"615fe94e1c0f6f1846ea6564639993049e73f1ca\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Make me a concise table with contexts and equations for entropy\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"86209fd8c40e215d9fd01e8b18472668b824d6c0\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Certainly! Here's a concise table summarizing the different contexts of entropy along with their respective equations:\\n\\n| Context               | Description                                              | Equation                                    |\\n|-----------------------|----------------------------------------------------------|---------------------------------------------|\\n| **Thermodynamics**    | Measures unavailable energy in a system.                 | $dS = \\\\frac{dQ_{\\\\text{rev}}}{T}$            |\\n| **Statistical Mechanics** | Number of microscopic configurations of a system.       | $S = k_B \\\\ln \\\\Omega$                        |\\n| **Information Theory**| Uncertainty or information content of a variable.        | $H(X) = -\\\\sum_{x \\\\in X} P(x) \\\\log_b P(x)$   |\\n| **Quantum Mechanics** | Uncertainty or mixedness of a quantum state.             | $S(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)$      |\\n| **Quantum Relative Entropy** | \\\"Distance\\\" between quantum states.                    | $S(\\\\rho || \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho - \\\\rho \\\\log \\\\sigma)$ |\\n| **R√©nyi Entropy**     | Generalization of quantum entropy.                      | $S_\\\\alpha(\\\\rho) = \\\\frac{1}{1 - \\\\alpha} \\\\log \\\\text{Tr}(\\\\rho^\\\\alpha)$ |\\n\\nThis table provides an overview of how entropy is conceptualized and calculated across different domains.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-fcacc156/2024-10-04_21-23-48-873247_8ff8b5f9.json\"\n",
      "    },\n",
      "    \"c48d86971937be01a3825806182a7347c866be33\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Display it as HTML instead\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"408d92c5350132feacfae5b339feff4ffe540b38\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Certainly! Here's the HTML representation of the table:\\n\\n```html\\n<table>\\n  <tr>\\n    <th>Context</th>\\n    <th>Description</th>\\n    <th>Equation</th>\\n  </tr>\\n  <tr>\\n   <td>Thermodynamics</td>\\n    <td>Measures unavailable energy in a system.</td>\\n    <td>dS = dQ<sub>rev</sub>/T</td>\\n  </tr>\\n  <tr>\\n    <td>Statistical Mechanics</td>\\n    <td>Number of microscopic configurations of a system.</td>\\n    <td>S = k<sub>B</sub> ln &Omega;</td>\\n  </tr>\\n  <tr>\\n    <td>Information Theory</td>\\n    <td>Uncertainty or information content of a variable.</td>\\n    <td>H(X) = -&sum;<sub>x &isin; X</sub> P(x) log<sub>b</sub> P(x)</td>\\n  </tr>\\n  <tr>\\n    <td>Quantum Mechanics</td>\\n    <td>Uncertainty or mixedness of a quantum state.</td>\\n    <td>S(&rho;) = -Tr(&rho; log &rho;)</td>\\n  </tr>\\n  <tr>\\n    <td>Quantum Relative Entropy</td>\\n    <td>\\\"Distance\\\" between quantum states.</td>\\n    <td>S(&rho; || &sigma;) = Tr(&rho; log &rho; - &rho; log &sigma;)</td>\\n  </tr>\\n  <tr>\\n    <td>R√©nyi Entropy</td>\\n    <td>Generalization of quantum entropy.</td>\\n    <td>S<sub>&alpha;</sub>(&rho;) = (1/(1 - &alpha;)) log Tr(&rho;<sup>&alpha;</sup>)</td>\\n  </tr>\\n</table>\\n```\\n\\nWhen rendered by an HTML processor, this will display a structured table with the various contexts and equations for entropy.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 2\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-6110eaf8/2024-10-04_21-42-44-329080_ac0779bb.json\"\n",
      "    },\n",
      "    \"e14cd50f1c58f516ae83bb975194107e6ac73264\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\\n\\n1. Thermodynamic Entropy:\\nIn thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\\n\\n$S = k_B \\\\ln W$\\n\\nWhere:\\n- S is entropy\\n- k_B is Boltzmann's constant\\n- W is the number of microstates (possible arrangements of particles)\\n\\nKey points:\\n- Entropy always increases in closed systems (2nd law of thermodynamics)\\n- It's related to the arrow of time and irreversibility\\n- Higher entropy = more disordered state\\n\\n2. Information Entropy:\\nIn information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\\n\\n$H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$\\n\\nWhere:\\n- H is information entropy\\n- p_i is the probability of the i-th message\\n- n is the number of possible messages\\n\\nKey points:\\n- Measured in bits (when using log2)\\n- Represents the minimum number of bits needed to encode a message\\n- Higher entropy = more uncertainty/information content\\n\\nConnections:\\n- Both concepts relate to uncertainty and unpredictability\\n- They're linked through statistical mechanics and the foundations of thermodynamics\\n- Both play crucial roles in fields like cryptography, data compression, and quantum computing\\n\\nIn essence, entropy is a measure of the spread of energy or information in a system, whether that system is physical or informational.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-304667c6/2024-10-04_21-17-32-007725_4deaa1c3.json\"\n",
      "    },\n",
      "    \"c83f99e3d4fe4c0d152282a438ec99e355ede69c\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\\n\\n1. Thermodynamic Entropy:\\nIn physics, entropy is a measure of the disorder or randomness in a system. It's defined as:\\n\\n$S = k_B \\\\ln W$\\n\\nWhere:\\n- S is entropy\\n- $k_B$ is Boltzmann's constant\\n- W is the number of microstates (possible arrangements of particles)\\n\\nKey points:\\n- Entropy always increases in closed systems (Second Law of Thermodynamics)\\n- It's related to the arrow of time and irreversibility of processes\\n- Higher entropy = more disorder = less available energy to do work\\n\\n2. Information Entropy:\\nIn information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\\n\\n$H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$\\n\\nWhere:\\n- H is information entropy\\n- $p_i$ is the probability of symbol i occurring\\n- n is the number of possible symbols\\n\\nKey points:\\n- Measured in bits (when using log base 2)\\n- Represents the minimum number of bits needed to encode a message\\n- Higher entropy = more uncertainty = more information content\\n\\nThe connection between these concepts lies in their relation to uncertainty and the distribution of states or outcomes in a system. Both forms of entropy are deeply connected to probability theory and statistical mechanics.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-304667c6/2024-10-04_21-21-02-603538_a5fbcccc.json\"\n",
      "    },\n",
      "    \"d4cdcbd8bd8b78657f99aef184ea2aced14b8794\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Here's a concise table with different contexts and their corresponding entropy equations:\\n\\n| Context | Equation | Description |\\n|---------|----------|-------------|\\n| Thermodynamics | $S = k_B \\\\ln W$ | Boltzmann entropy |\\n| Statistical mechanics | $S = -k_B \\\\sum_i p_i \\\\ln p_i$ | Gibbs entropy |\\n| Information theory | $H = -\\\\sum_i p_i \\\\log_2 p_i$ | Shannon entropy |\\n| Quantum mechanics | $S = -\\\\text{Tr}(\\\\rho \\\\ln \\\\rho)$ | von Neumann entropy |\\n| Continuous distributions | $h(X) = -\\\\int f(x) \\\\ln f(x) dx$ | Differential entropy |\\n| Relative entropy | $D_{KL}(P\\\\|Q) = \\\\sum_i P(i) \\\\ln \\\\frac{P(i)}{Q(i)}$ | Kullback-Leibler divergence |\\n\\nWhere:\\n- S, H: Entropy\\n- $k_B$: Boltzmann constant\\n- W: Number of microstates\\n- $p_i$: Probability of state i\\n- œÅ: Density matrix\\n- f(x): Probability density function\\n- P, Q: Probability distributions\\n\\nThis table covers the main equations for entropy across various fields in physics and information theory.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-304667c6/2024-10-04_21-23-47-319105_6939ae8c.json\"\n",
      "    },\n",
      "    \"08a163aa0eaf1aafb252a2607b5aa61b9caea3f6\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Here's the table displayed as HTML:\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th>Context</th>\\n      <th>Equation</th>\\n      <th>Description</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Thermodynamics</td>\\n      <td>S = k<sub>B</sub> ln W</td>\\n      <td>Boltzmann entropy</td>\\n    </tr>\\n    <tr>\\n      <td>Statistical mechanics</td>\\n      <td>S = -k<sub>B</sub> Œ£<sub>i</sub> p<sub>i</sub> ln p<sub>i</sub></td>\\n      <td>Gibbs entropy</td>\\n    </tr>\\n    <tr>\\n      <td>Information theory</td>\\n      <td>H = -Œ£<sub>i</sub> p<sub>i</sub> log<sub>2</sub> p<sub>i</sub></td>\\n      <td>Shannon entropy</td>\\n    </tr>\\n    <tr>\\n      <td>Quantum mechanics</td>\\n      <td>S = -Tr(œÅ ln œÅ)</td>\\n      <td>von Neumann entropy</td>\\n    </tr>\\n    <tr>\\n      <td>Continuous distributions</td>\\n      <td>h(X) = -‚à´ f(x) ln f(x) dx</td>\\n      <td>Differential entropy</td>\\n    </tr>\\n    <tr>\\n      <td>Relative entropy</td>\\n      <td>D<sub>KL</sub>(P||Q) = Œ£<sub>i</sub> P(i) ln [P(i)/Q(i)]</td>\\n      <td>Kullback-Leibler divergence</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n<p>Where:</p>\\n<ul>\\n  <li>S, H: Entropy</li>\\n  <li>k<sub>B</sub>: Boltzmann constant</li>\\n  <li>W: Number of microstates</li>\\n  <li>p<sub>i</sub>: Probability of state i</li>\\n  <li>œÅ: Density matrix</li>\\n  <li>f(x): Probability density function</li>\\n  <li>P, Q: Probability distributions</li>\\n</ul>\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-979d2168/2024-10-04_21-42-40-146973_0eb26084.json\"\n",
      "    }\n",
      "  },\n",
      "  \"agent_messages\": {\n",
      "    \"0\": [\n",
      "      \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\",\n",
      "      \"aedfff07ade343092500942d604f4b368f465752\",\n",
      "      \"d7e16e2196376e1cd583644a207b6c58cb41cf14\",\n",
      "      \"ec090ecf6d38522b4cfc5b79c465ff03ae79454d\",\n",
      "      \"c3a3124ef332cb68f937ea1888fbb57a7735c534\",\n",
      "      \"615fe94e1c0f6f1846ea6564639993049e73f1ca\",\n",
      "      \"86209fd8c40e215d9fd01e8b18472668b824d6c0\",\n",
      "      \"c48d86971937be01a3825806182a7347c866be33\",\n",
      "      \"408d92c5350132feacfae5b339feff4ffe540b38\"\n",
      "    ],\n",
      "    \"1\": [\n",
      "      \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\",\n",
      "      \"aedfff07ade343092500942d604f4b368f465752\",\n",
      "      [\n",
      "        \"e14cd50f1c58f516ae83bb975194107e6ac73264\",\n",
      "        \"c83f99e3d4fe4c0d152282a438ec99e355ede69c\"\n",
      "      ],\n",
      "      \"615fe94e1c0f6f1846ea6564639993049e73f1ca\",\n",
      "      \"d4cdcbd8bd8b78657f99aef184ea2aced14b8794\",\n",
      "      \"c48d86971937be01a3825806182a7347c866be33\",\n",
      "      \"08a163aa0eaf1aafb252a2607b5aa61b9caea3f6\"\n",
      "    ]\n",
      "  },\n",
      "  \"params\": {\n",
      "    \"0\": [\n",
      "      {},\n",
      "      {\n",
      "        \"temperature\": 0.8\n",
      "      },\n",
      "      {\n",
      "        \"max_tokens\": 1024\n",
      "      }\n",
      "    ],\n",
      "    \"1\": [\n",
      "      {\n",
      "        \"max_tokens\": 1024\n",
      "      },\n",
      "      {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "conv2_dict = conversation_to_dict(restored_c.agents)\n",
    "print(json.dumps(conv2_dict, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
