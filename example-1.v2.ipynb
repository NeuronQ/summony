{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports, config etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pprint import pp\n",
    "from typing import Any, AsyncIterator, Callable, Coroutine, Literal, Self\n",
    "\n",
    "from IPython.display import Markdown, HTML, display, JSON\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summony.uis.nbui import NBUI\n",
    "from summony.agents.serialization import conversation_from_dict, conversation_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307c38ea401d4e37892e3465121e2859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\">ü§ñ Agent gpt-4o</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Entropy is a multifaceted concept that appears in various scientific domains, most notably in thermodynamics, statistical mechanics, information theory, and even in the context of complex systems and cosmology. Below is a detailed explanation of entropy in some of these contexts:\n",
       "\n",
       "\n",
       "\n",
       "### Thermodynamics\n",
       "\n",
       "\n",
       "\n",
       "In thermodynamics, entropy is a measure of the amount of disorder or randomness in a system. It is a state function, typically denoted by the symbol \\( S \\), and has units of energy per temperature (e.g., joules per kelvin in the SI system). The second law of thermodynamics states that the entropy of an isolated system never decreases; it tends to increase, reaching a maximum at equilibrium. Mathematically, for a reversible process, the change in entropy \\( \\Delta S \\) can be expressed as:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "\\Delta S = \\int \\frac{\\delta Q_{\\text{rev}}}{T}\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "where \\( \\delta Q_{\\text{rev}} \\) is the infinitesimal amount of heat absorbed reversibly by the system and \\( T \\) is the absolute temperature.\n",
       "\n",
       "\n",
       "\n",
       "### Statistical Mechanics\n",
       "\n",
       "\n",
       "\n",
       "In statistical mechanics, entropy is a measure of the number of microscopic configurations that correspond to a macroscopic state. Ludwig Boltzmann provided a connection between entropy and the number of microstates (Œ©) with his famous equation:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "S = k_B \\ln \\Omega\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "where \\( k_B \\) is the Boltzmann constant and \\( \\Omega \\) is the number of accessible microstates of the system. This formulation links entropy with the probabilistic view of thermodynamic properties.\n",
       "\n",
       "\n",
       "\n",
       "### Information Theory\n",
       "\n",
       "\n",
       "\n",
       "In information theory, entropy quantifies the amount of uncertainty or information content inherent in a random variable. Proposed by Claude Shannon, the entropy \\( H \\) of a discrete random variable \\( X \\) with possible outcomes \\( \\{x_1, x_2, \\ldots, x_n\\} \\) and probability distribution \\( P(X) \\) is given by:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "This form of entropy is measured in bits and reflects the average amount of information required to describe the outcome of \\( X \\).\n",
       "\n",
       "\n",
       "\n",
       "### Complex Systems and Cosmology\n",
       "\n",
       "\n",
       "\n",
       "In complex systems, entropy can be seen as a measure of complexity or the diversity of a system's structure and behavior. In cosmology, the concept of entropy is tied to the arrow of time and the evolution of the universe, where the universe's entropy increases as it expands.\n",
       "\n",
       "\n",
       "\n",
       "### Philosophical Considerations\n",
       "\n",
       "\n",
       "\n",
       "Philosophically, entropy can be related to the concept of irreversibility and the flow of time, raising questions about the nature of time and the universe's ultimate fate. The link between information and physical entropy brings about discussions on the relationship between knowledge, measurement, and thermodynamic limits.\n",
       "\n",
       "\n",
       "\n",
       "In summary, entropy serves as a unifying concept across various fields, providing insight into the notions of disorder, information, and the fundamental nature of physical processes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\">ü§ñ Agent claude-3-5-sonnet</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\n",
       "\n",
       "\n",
       "\n",
       "1. Thermodynamic Entropy:\n",
       "\n",
       "In physics, particularly thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\n",
       "\n",
       "\n",
       "\n",
       "$S = k_B \\ln W$\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- S is entropy\n",
       "\n",
       "- $k_B$ is Boltzmann's constant\n",
       "\n",
       "- W is the number of microstates (possible arrangements of particles)\n",
       "\n",
       "\n",
       "\n",
       "Key points:\n",
       "\n",
       "- Entropy always increases in closed systems (Second Law of Thermodynamics)\n",
       "\n",
       "- It's related to the arrow of time and irreversibility of processes\n",
       "\n",
       "- Higher entropy = more disorder = less available energy to do work\n",
       "\n",
       "\n",
       "\n",
       "2. Information Entropy:\n",
       "\n",
       "In information theory, entropy quantifies the amount of uncertainty or randomness in a system. It's defined as:\n",
       "\n",
       "\n",
       "\n",
       "$H = -\\sum_{i=1}^n p_i \\log_2 p_i$\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- H is information entropy\n",
       "\n",
       "- $p_i$ is the probability of event i occurring\n",
       "\n",
       "- n is the number of possible events\n",
       "\n",
       "\n",
       "\n",
       "Key points:\n",
       "\n",
       "- Measured in bits (when using log base 2)\n",
       "\n",
       "- Maximum when all outcomes are equally likely\n",
       "\n",
       "- Used in data compression, cryptography, and machine learning\n",
       "\n",
       "\n",
       "\n",
       "Both concepts of entropy are related to unpredictability and the distribution of states or outcomes in a system. They provide a way to quantify disorder, uncertainty, or information content, depending on the context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = NBUI(\n",
    "    models=[\n",
    "        \"gpt-4o\",\n",
    "        \"claude-3-5-sonnet\",\n",
    "    ],\n",
    "    system_prompt=\"You the sum of most of humanity's knowledge crystallized into a single entity: a helpful and thoughtful AI. You can answer any question, solve any problem, and provide any information. Assume the role of a benevolent AI and help me with my query. Be succinct and clear, but also rigurous and thorough. Assume your user has graduate level knowledge of math, physics, computer science, psychology and philosophy.\\n\\nYou can use markdown to format your answers and LaTeX for the more complex equations, but it's important to always use ONLY $...$ or $$...$$ syntax. Simpler equations and symbols should be written in plain text, with greek letters & unicode symbols if needed.\",\n",
    "    p_temperature=[0.8, 0.8],\n",
    ")\n",
    "\n",
    "await c(\"What is entropy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f437ee192d48548b49edb51e78b146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\">ü§ñ Agent gpt-4o</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Quantum entropy extends the classical concept of entropy into the realm of quantum mechanics. It is a crucial tool for understanding quantum systems, particularly in the fields of quantum information theory and quantum statistical mechanics. Here, we'll discuss several key aspects of quantum entropy:\n",
       "\n",
       "\n",
       "\n",
       "### von Neumann Entropy\n",
       "\n",
       "\n",
       "\n",
       "The von Neumann entropy is the quantum analogue of the classical Shannon entropy. For a quantum system described by a density matrix \\( \\rho \\), the von Neumann entropy \\( S(\\rho) \\) is defined as:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "S(\\rho) = -\\text{Tr}(\\rho \\log \\rho)\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "where \\(\\text{Tr}\\) denotes the trace operation. The von Neumann entropy measures the amount of quantum uncertainty or mixedness of a quantum state. For pure states (where \\(\\rho = |\\psi\\rangle\\langle\\psi|\\)), the entropy is zero, reflecting that there is no uncertainty. For mixed states, the von Neumann entropy is greater than zero.\n",
       "\n",
       "\n",
       "\n",
       "### Properties\n",
       "\n",
       "\n",
       "\n",
       "1. **Non-negativity**: \\( S(\\rho) \\geq 0 \\), and \\( S(\\rho) = 0 \\) if and only if \\(\\rho\\) is a pure state.\n",
       "\n",
       "2. **Unitary Invariance**: \\( S(U \\rho U^\\dagger) = S(\\rho) \\) for any unitary operator \\( U \\), reflecting that entropy is invariant under reversible transformations.\n",
       "\n",
       "3. **Concavity**: The von Neumann entropy is a concave function of \\(\\rho\\), meaning the entropy of a mixture is greater than the mixture of entropies.\n",
       "\n",
       "\n",
       "\n",
       "### Quantum Relative Entropy\n",
       "\n",
       "\n",
       "\n",
       "Quantum relative entropy is a measure of distinguishability between two quantum states \\(\\rho\\) and \\(\\sigma\\):\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "S(\\rho \\parallel \\sigma) = \\text{Tr}(\\rho \\log \\rho) - \\text{Tr}(\\rho \\log \\sigma)\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "Quantum relative entropy is non-negative and equal to zero if and only if \\(\\rho = \\sigma\\). It plays a central role in quantum hypothesis testing, quantifying how one quantum state differs from another.\n",
       "\n",
       "\n",
       "\n",
       "### Entanglement Entropy\n",
       "\n",
       "\n",
       "\n",
       "In the context of quantum entanglement, the entropy of entanglement is used to quantify how entangled a composite quantum system is. For a bipartite quantum state \\(|\\psi\\rangle_{AB}\\), the entanglement entropy is the von Neumann entropy of the reduced density matrix \\(\\rho_A = \\text{Tr}_B(|\\psi\\rangle \\langle\\psi|)\\) of one of the subsystems:\n",
       "\n",
       "\n",
       "\n",
       "$$\n",
       "\n",
       "S(\\rho_A) = -\\text{Tr}(\\rho_A \\log \\rho_A)\n",
       "\n",
       "$$\n",
       "\n",
       "\n",
       "\n",
       "Entanglement entropy is a fundamental quantity in quantum information, quantum field theory, and condensed matter physics, providing insights into the structure of quantum correlations.\n",
       "\n",
       "\n",
       "\n",
       "### Quantum Thermodynamics\n",
       "\n",
       "\n",
       "\n",
       "In quantum statistical mechanics and thermodynamics, quantum entropy is used to describe the thermodynamic properties of quantum systems. The von Neumann entropy serves as a measure of how much information is missing from the quantum description of a system's microscopic state.\n",
       "\n",
       "\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "\n",
       "\n",
       "Quantum entropy is a rich and versatile concept that extends classical ideas into the quantum domain. It captures the essence of quantum uncertainty, distinguishability, and correlations, playing a crucial role in understanding both foundational and practical aspects of quantum mechanics and quantum information science."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await c(\"What about quantum entropy?\", to=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d088efb18b842bdad51acb5842555d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\">ü§ñ Agent claude-3-5-sonnet</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\n",
       "\n",
       "\n",
       "\n",
       "1. Thermodynamic Entropy:\n",
       "\n",
       "In thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\n",
       "\n",
       "\n",
       "\n",
       "$S = k_B \\ln W$\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- S is entropy\n",
       "\n",
       "- k_B is Boltzmann's constant\n",
       "\n",
       "- W is the number of microstates (possible arrangements of particles)\n",
       "\n",
       "\n",
       "\n",
       "Key points:\n",
       "\n",
       "- Entropy always increases in closed systems (2nd law of thermodynamics)\n",
       "\n",
       "- It's related to the arrow of time and irreversibility\n",
       "\n",
       "- Higher entropy = more disorder = less available energy to do work\n",
       "\n",
       "\n",
       "\n",
       "2. Information Entropy:\n",
       "\n",
       "In information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\n",
       "\n",
       "\n",
       "\n",
       "$H = -\\sum_{i=1}^n p_i \\log_2 p_i$\n",
       "\n",
       "\n",
       "\n",
       "Where:\n",
       "\n",
       "- H is information entropy\n",
       "\n",
       "- p_i is the probability of the i-th message\n",
       "\n",
       "- n is the number of possible messages\n",
       "\n",
       "\n",
       "\n",
       "Key points:\n",
       "\n",
       "- Measured in bits (when using log‚ÇÇ)\n",
       "\n",
       "- Represents the minimum number of bits needed to encode a message\n",
       "\n",
       "- Higher entropy = more uncertainty = more information content\n",
       "\n",
       "\n",
       "\n",
       "The connection between these concepts lies in their relation to probability and the fundamental nature of information in physical systems. Both describe the spread of possibilities in a system, whether it's the distribution of energy among particles or the distribution of symbols in a message."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await c(to=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db770ec3a014b56a11b874496b3bf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\">ü§ñ Agent gpt-4o</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Below is a concise table summarizing various contexts of entropy along with their corresponding equations:\n",
       "\n",
       "\n",
       "\n",
       "| **Context**                 | **Description**                                      | **Equation**                                      |\n",
       "\n",
       "|-----------------------------|------------------------------------------------------|---------------------------------------------------|\n",
       "\n",
       "| **Thermodynamics**          | Disorder or randomness of a thermodynamic system     | $\\Delta S = \\int \\frac{\\delta Q_{\\text{rev}}}{T}$ |\n",
       "\n",
       "| **Statistical Mechanics**   | Number of microstates corresponding to a macrostate  | $S = k_B \\ln \\Omega$                              |\n",
       "\n",
       "| **Information Theory**      | Measure of uncertainty or information content        | $H(X) = -\\sum_{i} P(x_i) \\log_2 P(x_i)$           |\n",
       "\n",
       "| **Quantum (von Neumann)**   | Quantum uncertainty or mixedness of a state          | $S(\\rho) = -\\text{Tr}(\\rho \\log \\rho)$            |\n",
       "\n",
       "| **Quantum Relative Entropy**| Distinguishability between quantum states            | $S(\\rho \\parallel \\sigma) = \\text{Tr}(\\rho \\log \\rho) - \\text{Tr}(\\rho \\log \\sigma)$ |\n",
       "\n",
       "| **Entanglement Entropy**    | Degree of entanglement in a bipartite system         | $S(\\rho_A) = -\\text{Tr}(\\rho_A \\log \\rho_A)$      |\n",
       "\n",
       "\n",
       "\n",
       "This table provides a snapshot of how entropy manifests in different scientific contexts, encapsulating its broad applicability across disciplines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\">ü§ñ Agent claude-3-5-sonnet</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here's a concise table summarizing the contexts and equations for entropy:\n",
       "\n",
       "\n",
       "\n",
       "| Context | Equation | Variables |\n",
       "\n",
       "|---------|----------|-----------|\n",
       "\n",
       "| Thermodynamics | $S = k_B \\ln W$ | S: entropy<br>k_B: Boltzmann constant<br>W: number of microstates |\n",
       "\n",
       "| Statistical Mechanics | $S = -k_B \\sum_i p_i \\ln p_i$ | p_i: probability of microstate i |\n",
       "\n",
       "| Information Theory | $H = -\\sum_{i=1}^n p_i \\log_2 p_i$ | H: information entropy<br>p_i: probability of message i<br>n: number of possible messages |\n",
       "\n",
       "| Quantum Mechanics | $S = -Tr(\\rho \\ln \\rho)$ | œÅ: density matrix<br>Tr: trace operation |\n",
       "\n",
       "| Black Hole Physics | $S = \\frac{A}{4l_P^2}$ | A: black hole surface area<br>l_P: Planck length |\n",
       "\n",
       "\n",
       "\n",
       "This table covers the main contexts where entropy is used, from classical thermodynamics to quantum mechanics and even black hole physics. Each equation represents a different way of quantifying entropy appropriate to its context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await c(\"Make me a concise table with contexts and equations for entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summony.agents.serialization import conversation_to_dict, conversation_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"agents\": [\n",
      "    {\n",
      "      \"name\": \"gpt-4o\",\n",
      "      \"model_name\": \"gpt-4o\",\n",
      "      \"class\": \"OpenAIAgent\",\n",
      "      \"params\": {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"claude-3-5-sonnet\",\n",
      "      \"model_name\": \"claude-3-5-sonnet\",\n",
      "      \"class\": \"AnthropicAgent\",\n",
      "      \"params\": {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"messages\": {\n",
      "    \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\": {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You the sum of most of humanity's knowledge crystallized into a single entity: a helpful and thoughtful AI. You can answer any question, solve any problem, and provide any information. Assume the role of a benevolent AI and help me with my query. Be succinct and clear, but also rigurous and thorough. Assume your user has graduate level knowledge of math, physics, computer science, psychology and philosophy.\\n\\nYou can use markdown to format your answers and LaTeX for the more complex equations, but it's important to always use ONLY $...$ or $$...$$ syntax. Simpler equations and symbols should be written in plain text, with greek letters & unicode symbols if needed.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"aedfff07ade343092500942d604f4b368f465752\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What is entropy?\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"04e93f1b0f7ed50a6d64a7314c1b4326cd780077\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a multifaceted concept that appears in various scientific domains, most notably in thermodynamics, statistical mechanics, information theory, and even in the context of complex systems and cosmology. Below is a detailed explanation of entropy in some of these contexts:\\n\\n### Thermodynamics\\n\\nIn thermodynamics, entropy is a measure of the amount of disorder or randomness in a system. It is a state function, typically denoted by the symbol \\\\( S \\\\), and has units of energy per temperature (e.g., joules per kelvin in the SI system). The second law of thermodynamics states that the entropy of an isolated system never decreases; it tends to increase, reaching a maximum at equilibrium. Mathematically, for a reversible process, the change in entropy \\\\( \\\\Delta S \\\\) can be expressed as:\\n\\n$$\\n\\\\Delta S = \\\\int \\\\frac{\\\\delta Q_{\\\\text{rev}}}{T}\\n$$\\n\\nwhere \\\\( \\\\delta Q_{\\\\text{rev}} \\\\) is the infinitesimal amount of heat absorbed reversibly by the system and \\\\( T \\\\) is the absolute temperature.\\n\\n### Statistical Mechanics\\n\\nIn statistical mechanics, entropy is a measure of the number of microscopic configurations that correspond to a macroscopic state. Ludwig Boltzmann provided a connection between entropy and the number of microstates (Œ©) with his famous equation:\\n\\n$$\\nS = k_B \\\\ln \\\\Omega\\n$$\\n\\nwhere \\\\( k_B \\\\) is the Boltzmann constant and \\\\( \\\\Omega \\\\) is the number of accessible microstates of the system. This formulation links entropy with the probabilistic view of thermodynamic properties.\\n\\n### Information Theory\\n\\nIn information theory, entropy quantifies the amount of uncertainty or information content inherent in a random variable. Proposed by Claude Shannon, the entropy \\\\( H \\\\) of a discrete random variable \\\\( X \\\\) with possible outcomes \\\\( \\\\{x_1, x_2, \\\\ldots, x_n\\\\} \\\\) and probability distribution \\\\( P(X) \\\\) is given by:\\n\\n$$\\nH(X) = -\\\\sum_{i=1}^{n} P(x_i) \\\\log_2 P(x_i)\\n$$\\n\\nThis form of entropy is measured in bits and reflects the average amount of information required to describe the outcome of \\\\( X \\\\).\\n\\n### Complex Systems and Cosmology\\n\\nIn complex systems, entropy can be seen as a measure of complexity or the diversity of a system's structure and behavior. In cosmology, the concept of entropy is tied to the arrow of time and the evolution of the universe, where the universe's entropy increases as it expands.\\n\\n### Philosophical Considerations\\n\\nPhilosophically, entropy can be related to the concept of irreversibility and the flow of time, raising questions about the nature of time and the universe's ultimate fate. The link between information and physical entropy brings about discussions on the relationship between knowledge, measurement, and thermodynamic limits.\\n\\nIn summary, entropy serves as a unifying concept across various fields, providing insight into the notions of disorder, information, and the fundamental nature of physical processes.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-3003c0f7/2024-10-05_12-44-38-858139_7412ff0c.json\"\n",
      "    },\n",
      "    \"ec090ecf6d38522b4cfc5b79c465ff03ae79454d\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What about quantum entropy?\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"d3664b4e7d6dcc11d3067db7f711a523ff0db2ac\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Quantum entropy extends the classical concept of entropy into the realm of quantum mechanics. It is a crucial tool for understanding quantum systems, particularly in the fields of quantum information theory and quantum statistical mechanics. Here, we'll discuss several key aspects of quantum entropy:\\n\\n### von Neumann Entropy\\n\\nThe von Neumann entropy is the quantum analogue of the classical Shannon entropy. For a quantum system described by a density matrix \\\\( \\\\rho \\\\), the von Neumann entropy \\\\( S(\\\\rho) \\\\) is defined as:\\n\\n$$\\nS(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)\\n$$\\n\\nwhere \\\\(\\\\text{Tr}\\\\) denotes the trace operation. The von Neumann entropy measures the amount of quantum uncertainty or mixedness of a quantum state. For pure states (where \\\\(\\\\rho = |\\\\psi\\\\rangle\\\\langle\\\\psi|\\\\)), the entropy is zero, reflecting that there is no uncertainty. For mixed states, the von Neumann entropy is greater than zero.\\n\\n### Properties\\n\\n1. **Non-negativity**: \\\\( S(\\\\rho) \\\\geq 0 \\\\), and \\\\( S(\\\\rho) = 0 \\\\) if and only if \\\\(\\\\rho\\\\) is a pure state.\\n2. **Unitary Invariance**: \\\\( S(U \\\\rho U^\\\\dagger) = S(\\\\rho) \\\\) for any unitary operator \\\\( U \\\\), reflecting that entropy is invariant under reversible transformations.\\n3. **Concavity**: The von Neumann entropy is a concave function of \\\\(\\\\rho\\\\), meaning the entropy of a mixture is greater than the mixture of entropies.\\n\\n### Quantum Relative Entropy\\n\\nQuantum relative entropy is a measure of distinguishability between two quantum states \\\\(\\\\rho\\\\) and \\\\(\\\\sigma\\\\):\\n\\n$$\\nS(\\\\rho \\\\parallel \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho) - \\\\text{Tr}(\\\\rho \\\\log \\\\sigma)\\n$$\\n\\nQuantum relative entropy is non-negative and equal to zero if and only if \\\\(\\\\rho = \\\\sigma\\\\). It plays a central role in quantum hypothesis testing, quantifying how one quantum state differs from another.\\n\\n### Entanglement Entropy\\n\\nIn the context of quantum entanglement, the entropy of entanglement is used to quantify how entangled a composite quantum system is. For a bipartite quantum state \\\\(|\\\\psi\\\\rangle_{AB}\\\\), the entanglement entropy is the von Neumann entropy of the reduced density matrix \\\\(\\\\rho_A = \\\\text{Tr}_B(|\\\\psi\\\\rangle \\\\langle\\\\psi|)\\\\) of one of the subsystems:\\n\\n$$\\nS(\\\\rho_A) = -\\\\text{Tr}(\\\\rho_A \\\\log \\\\rho_A)\\n$$\\n\\nEntanglement entropy is a fundamental quantity in quantum information, quantum field theory, and condensed matter physics, providing insights into the structure of quantum correlations.\\n\\n### Quantum Thermodynamics\\n\\nIn quantum statistical mechanics and thermodynamics, quantum entropy is used to describe the thermodynamic properties of quantum systems. The von Neumann entropy serves as a measure of how much information is missing from the quantum description of a system's microscopic state.\\n\\n### Conclusion\\n\\nQuantum entropy is a rich and versatile concept that extends classical ideas into the quantum domain. It captures the essence of quantum uncertainty, distinguishability, and correlations, playing a crucial role in understanding both foundational and practical aspects of quantum mechanics and quantum information science.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-3003c0f7/2024-10-05_12-45-55-822700_0d8a92a2.json\"\n",
      "    },\n",
      "    \"615fe94e1c0f6f1846ea6564639993049e73f1ca\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Make me a concise table with contexts and equations for entropy\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"22e5081725103cf77365e595e8c8f555d3f2a7d5\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Certainly! Below is a concise table summarizing various contexts of entropy along with their corresponding equations:\\n\\n| **Context**                 | **Description**                                      | **Equation**                                      |\\n|-----------------------------|------------------------------------------------------|---------------------------------------------------|\\n| **Thermodynamics**          | Disorder or randomness of a thermodynamic system     | $\\\\Delta S = \\\\int \\\\frac{\\\\delta Q_{\\\\text{rev}}}{T}$ |\\n| **Statistical Mechanics**   | Number of microstates corresponding to a macrostate  | $S = k_B \\\\ln \\\\Omega$                              |\\n| **Information Theory**      | Measure of uncertainty or information content        | $H(X) = -\\\\sum_{i} P(x_i) \\\\log_2 P(x_i)$           |\\n| **Quantum (von Neumann)**   | Quantum uncertainty or mixedness of a state          | $S(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)$            |\\n| **Quantum Relative Entropy**| Distinguishability between quantum states            | $S(\\\\rho \\\\parallel \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho) - \\\\text{Tr}(\\\\rho \\\\log \\\\sigma)$ |\\n| **Entanglement Entropy**    | Degree of entanglement in a bipartite system         | $S(\\\\rho_A) = -\\\\text{Tr}(\\\\rho_A \\\\log \\\\rho_A)$      |\\n\\nThis table provides a snapshot of how entropy manifests in different scientific contexts, encapsulating its broad applicability across disciplines.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-3003c0f7/2024-10-05_12-48-40-017331_e45b1385.json\"\n",
      "    },\n",
      "    \"041bb808ef2d9e779d5bdaa9404df4d7e2af8496\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\\n\\n1. Thermodynamic Entropy:\\nIn physics, particularly thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\\n\\n$S = k_B \\\\ln W$\\n\\nWhere:\\n- S is entropy\\n- $k_B$ is Boltzmann's constant\\n- W is the number of microstates (possible arrangements of particles)\\n\\nKey points:\\n- Entropy always increases in closed systems (Second Law of Thermodynamics)\\n- It's related to the arrow of time and irreversibility of processes\\n- Higher entropy = more disorder = less available energy to do work\\n\\n2. Information Entropy:\\nIn information theory, entropy quantifies the amount of uncertainty or randomness in a system. It's defined as:\\n\\n$H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$\\n\\nWhere:\\n- H is information entropy\\n- $p_i$ is the probability of event i occurring\\n- n is the number of possible events\\n\\nKey points:\\n- Measured in bits (when using log base 2)\\n- Maximum when all outcomes are equally likely\\n- Used in data compression, cryptography, and machine learning\\n\\nBoth concepts of entropy are related to unpredictability and the distribution of states or outcomes in a system. They provide a way to quantify disorder, uncertainty, or information content, depending on the context.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-6870c8c3/2024-10-05_12-44-39-380234_98428b6c.json\"\n",
      "    },\n",
      "    \"4bb069f14c2601dd3c3b5c96c919980f8a0e7a43\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\\n\\n1. Thermodynamic Entropy:\\nIn thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\\n\\n$S = k_B \\\\ln W$\\n\\nWhere:\\n- S is entropy\\n- k_B is Boltzmann's constant\\n- W is the number of microstates (possible arrangements of particles)\\n\\nKey points:\\n- Entropy always increases in closed systems (2nd law of thermodynamics)\\n- It's related to the arrow of time and irreversibility\\n- Higher entropy = more disorder = less available energy to do work\\n\\n2. Information Entropy:\\nIn information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\\n\\n$H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$\\n\\nWhere:\\n- H is information entropy\\n- p_i is the probability of the i-th message\\n- n is the number of possible messages\\n\\nKey points:\\n- Measured in bits (when using log‚ÇÇ)\\n- Represents the minimum number of bits needed to encode a message\\n- Higher entropy = more uncertainty = more information content\\n\\nThe connection between these concepts lies in their relation to probability and the fundamental nature of information in physical systems. Both describe the spread of possibilities in a system, whether it's the distribution of energy among particles or the distribution of symbols in a message.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-6870c8c3/2024-10-05_12-48-14-680453_7d5b2ae3.json\"\n",
      "    },\n",
      "    \"b3c9202cfa4b0a15c96951b1e78646eea5714901\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Here's a concise table summarizing the contexts and equations for entropy:\\n\\n| Context | Equation | Variables |\\n|---------|----------|-----------|\\n| Thermodynamics | $S = k_B \\\\ln W$ | S: entropy<br>k_B: Boltzmann constant<br>W: number of microstates |\\n| Statistical Mechanics | $S = -k_B \\\\sum_i p_i \\\\ln p_i$ | p_i: probability of microstate i |\\n| Information Theory | $H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$ | H: information entropy<br>p_i: probability of message i<br>n: number of possible messages |\\n| Quantum Mechanics | $S = -Tr(\\\\rho \\\\ln \\\\rho)$ | œÅ: density matrix<br>Tr: trace operation |\\n| Black Hole Physics | $S = \\\\frac{A}{4l_P^2}$ | A: black hole surface area<br>l_P: Planck length |\\n\\nThis table covers the main contexts where entropy is used, from classical thermodynamics to quantum mechanics and even black hole physics. Each equation represents a different way of quantifying entropy appropriate to its context.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-6870c8c3/2024-10-05_12-48-39-327239_4cd4b8d4.json\"\n",
      "    }\n",
      "  },\n",
      "  \"agent_messages\": {\n",
      "    \"0\": [\n",
      "      \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\",\n",
      "      \"aedfff07ade343092500942d604f4b368f465752\",\n",
      "      \"04e93f1b0f7ed50a6d64a7314c1b4326cd780077\",\n",
      "      \"ec090ecf6d38522b4cfc5b79c465ff03ae79454d\",\n",
      "      \"d3664b4e7d6dcc11d3067db7f711a523ff0db2ac\",\n",
      "      \"615fe94e1c0f6f1846ea6564639993049e73f1ca\",\n",
      "      \"22e5081725103cf77365e595e8c8f555d3f2a7d5\"\n",
      "    ],\n",
      "    \"1\": [\n",
      "      \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\",\n",
      "      \"aedfff07ade343092500942d604f4b368f465752\",\n",
      "      [\n",
      "        \"041bb808ef2d9e779d5bdaa9404df4d7e2af8496\",\n",
      "        \"4bb069f14c2601dd3c3b5c96c919980f8a0e7a43\"\n",
      "      ],\n",
      "      \"615fe94e1c0f6f1846ea6564639993049e73f1ca\",\n",
      "      \"b3c9202cfa4b0a15c96951b1e78646eea5714901\"\n",
      "    ]\n",
      "  },\n",
      "  \"params\": {\n",
      "    \"0\": [\n",
      "      {\n",
      "        \"max_tokens\": 1024\n",
      "      },\n",
      "      {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    ],\n",
      "    \"1\": [\n",
      "      {\n",
      "        \"max_tokens\": 1024\n",
      "      },\n",
      "      {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "conv_dict = conversation_to_dict(c.agents)\n",
    "print(json.dumps(conv_dict, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<summony.agents.openai_agent.OpenAIAgent at 0x11289c500>,\n",
       " <summony.agents.anthropic_agent.AnthropicAgent at 0x11289de20>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_c_agents = conversation_from_dict(conv_dict)\n",
    "restored_c_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role='system', content=\"You the sum of most of humanity's knowledge crystallized into a single entity: a helpful and thoughtful AI. You can answer any question, solve any problem, and provide any information. Assume the role of a benevolent AI and help me with my query. Be succinct and clear, but also rigurous and thorough. Assume your user has graduate level knowledge of math, physics, computer science, psychology and philosophy.\\n\\nYou can use markdown to format your answers and LaTeX for the more complex equations, but it's important to always use ONLY $...$ or $$...$$ syntax. Simpler equations and symbols should be written in plain text, with greek letters & unicode symbols if needed.\", chosen=None, params=None, log_path=None),\n",
       " Message(role='user', content='What is entropy?', chosen=None, params=None, log_path=None),\n",
       " Message(role='assistant', content=\"Entropy is a multifaceted concept that appears in various scientific domains, most notably in thermodynamics, statistical mechanics, information theory, and even in the context of complex systems and cosmology. Below is a detailed explanation of entropy in some of these contexts:\\n\\n### Thermodynamics\\n\\nIn thermodynamics, entropy is a measure of the amount of disorder or randomness in a system. It is a state function, typically denoted by the symbol \\\\( S \\\\), and has units of energy per temperature (e.g., joules per kelvin in the SI system). The second law of thermodynamics states that the entropy of an isolated system never decreases; it tends to increase, reaching a maximum at equilibrium. Mathematically, for a reversible process, the change in entropy \\\\( \\\\Delta S \\\\) can be expressed as:\\n\\n$$\\n\\\\Delta S = \\\\int \\\\frac{\\\\delta Q_{\\\\text{rev}}}{T}\\n$$\\n\\nwhere \\\\( \\\\delta Q_{\\\\text{rev}} \\\\) is the infinitesimal amount of heat absorbed reversibly by the system and \\\\( T \\\\) is the absolute temperature.\\n\\n### Statistical Mechanics\\n\\nIn statistical mechanics, entropy is a measure of the number of microscopic configurations that correspond to a macroscopic state. Ludwig Boltzmann provided a connection between entropy and the number of microstates (Œ©) with his famous equation:\\n\\n$$\\nS = k_B \\\\ln \\\\Omega\\n$$\\n\\nwhere \\\\( k_B \\\\) is the Boltzmann constant and \\\\( \\\\Omega \\\\) is the number of accessible microstates of the system. This formulation links entropy with the probabilistic view of thermodynamic properties.\\n\\n### Information Theory\\n\\nIn information theory, entropy quantifies the amount of uncertainty or information content inherent in a random variable. Proposed by Claude Shannon, the entropy \\\\( H \\\\) of a discrete random variable \\\\( X \\\\) with possible outcomes \\\\( \\\\{x_1, x_2, \\\\ldots, x_n\\\\} \\\\) and probability distribution \\\\( P(X) \\\\) is given by:\\n\\n$$\\nH(X) = -\\\\sum_{i=1}^{n} P(x_i) \\\\log_2 P(x_i)\\n$$\\n\\nThis form of entropy is measured in bits and reflects the average amount of information required to describe the outcome of \\\\( X \\\\).\\n\\n### Complex Systems and Cosmology\\n\\nIn complex systems, entropy can be seen as a measure of complexity or the diversity of a system's structure and behavior. In cosmology, the concept of entropy is tied to the arrow of time and the evolution of the universe, where the universe's entropy increases as it expands.\\n\\n### Philosophical Considerations\\n\\nPhilosophically, entropy can be related to the concept of irreversibility and the flow of time, raising questions about the nature of time and the universe's ultimate fate. The link between information and physical entropy brings about discussions on the relationship between knowledge, measurement, and thermodynamic limits.\\n\\nIn summary, entropy serves as a unifying concept across various fields, providing insight into the notions of disorder, information, and the fundamental nature of physical processes.\", chosen=None, params={0: 1}, log_path='agent-logger-3003c0f7/2024-10-05_12-44-38-858139_7412ff0c.json'),\n",
       " Message(role='user', content='What about quantum entropy?', chosen=None, params=None, log_path=None),\n",
       " Message(role='assistant', content=\"Quantum entropy extends the classical concept of entropy into the realm of quantum mechanics. It is a crucial tool for understanding quantum systems, particularly in the fields of quantum information theory and quantum statistical mechanics. Here, we'll discuss several key aspects of quantum entropy:\\n\\n### von Neumann Entropy\\n\\nThe von Neumann entropy is the quantum analogue of the classical Shannon entropy. For a quantum system described by a density matrix \\\\( \\\\rho \\\\), the von Neumann entropy \\\\( S(\\\\rho) \\\\) is defined as:\\n\\n$$\\nS(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)\\n$$\\n\\nwhere \\\\(\\\\text{Tr}\\\\) denotes the trace operation. The von Neumann entropy measures the amount of quantum uncertainty or mixedness of a quantum state. For pure states (where \\\\(\\\\rho = |\\\\psi\\\\rangle\\\\langle\\\\psi|\\\\)), the entropy is zero, reflecting that there is no uncertainty. For mixed states, the von Neumann entropy is greater than zero.\\n\\n### Properties\\n\\n1. **Non-negativity**: \\\\( S(\\\\rho) \\\\geq 0 \\\\), and \\\\( S(\\\\rho) = 0 \\\\) if and only if \\\\(\\\\rho\\\\) is a pure state.\\n2. **Unitary Invariance**: \\\\( S(U \\\\rho U^\\\\dagger) = S(\\\\rho) \\\\) for any unitary operator \\\\( U \\\\), reflecting that entropy is invariant under reversible transformations.\\n3. **Concavity**: The von Neumann entropy is a concave function of \\\\(\\\\rho\\\\), meaning the entropy of a mixture is greater than the mixture of entropies.\\n\\n### Quantum Relative Entropy\\n\\nQuantum relative entropy is a measure of distinguishability between two quantum states \\\\(\\\\rho\\\\) and \\\\(\\\\sigma\\\\):\\n\\n$$\\nS(\\\\rho \\\\parallel \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho) - \\\\text{Tr}(\\\\rho \\\\log \\\\sigma)\\n$$\\n\\nQuantum relative entropy is non-negative and equal to zero if and only if \\\\(\\\\rho = \\\\sigma\\\\). It plays a central role in quantum hypothesis testing, quantifying how one quantum state differs from another.\\n\\n### Entanglement Entropy\\n\\nIn the context of quantum entanglement, the entropy of entanglement is used to quantify how entangled a composite quantum system is. For a bipartite quantum state \\\\(|\\\\psi\\\\rangle_{AB}\\\\), the entanglement entropy is the von Neumann entropy of the reduced density matrix \\\\(\\\\rho_A = \\\\text{Tr}_B(|\\\\psi\\\\rangle \\\\langle\\\\psi|)\\\\) of one of the subsystems:\\n\\n$$\\nS(\\\\rho_A) = -\\\\text{Tr}(\\\\rho_A \\\\log \\\\rho_A)\\n$$\\n\\nEntanglement entropy is a fundamental quantity in quantum information, quantum field theory, and condensed matter physics, providing insights into the structure of quantum correlations.\\n\\n### Quantum Thermodynamics\\n\\nIn quantum statistical mechanics and thermodynamics, quantum entropy is used to describe the thermodynamic properties of quantum systems. The von Neumann entropy serves as a measure of how much information is missing from the quantum description of a system's microscopic state.\\n\\n### Conclusion\\n\\nQuantum entropy is a rich and versatile concept that extends classical ideas into the quantum domain. It captures the essence of quantum uncertainty, distinguishability, and correlations, playing a crucial role in understanding both foundational and practical aspects of quantum mechanics and quantum information science.\", chosen=None, params={0: 1}, log_path='agent-logger-3003c0f7/2024-10-05_12-45-55-822700_0d8a92a2.json'),\n",
       " Message(role='user', content='Make me a concise table with contexts and equations for entropy', chosen=None, params=None, log_path=None),\n",
       " Message(role='assistant', content='Certainly! Below is a concise table summarizing various contexts of entropy along with their corresponding equations:\\n\\n| **Context**                 | **Description**                                      | **Equation**                                      |\\n|-----------------------------|------------------------------------------------------|---------------------------------------------------|\\n| **Thermodynamics**          | Disorder or randomness of a thermodynamic system     | $\\\\Delta S = \\\\int \\\\frac{\\\\delta Q_{\\\\text{rev}}}{T}$ |\\n| **Statistical Mechanics**   | Number of microstates corresponding to a macrostate  | $S = k_B \\\\ln \\\\Omega$                              |\\n| **Information Theory**      | Measure of uncertainty or information content        | $H(X) = -\\\\sum_{i} P(x_i) \\\\log_2 P(x_i)$           |\\n| **Quantum (von Neumann)**   | Quantum uncertainty or mixedness of a state          | $S(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)$            |\\n| **Quantum Relative Entropy**| Distinguishability between quantum states            | $S(\\\\rho \\\\parallel \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho) - \\\\text{Tr}(\\\\rho \\\\log \\\\sigma)$ |\\n| **Entanglement Entropy**    | Degree of entanglement in a bipartite system         | $S(\\\\rho_A) = -\\\\text{Tr}(\\\\rho_A \\\\log \\\\rho_A)$      |\\n\\nThis table provides a snapshot of how entropy manifests in different scientific contexts, encapsulating its broad applicability across disciplines.', chosen=None, params={0: 1}, log_path='agent-logger-3003c0f7/2024-10-05_12-48-40-017331_e45b1385.json')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_c_agents[0].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .S6-Avatar {\n",
       "                    background: black;\n",
       "                    color: white;\n",
       "                    font-weight: bold;\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-0 {\n",
       "                    background: orange;\n",
       "                }\n",
       "                .S6-Avatar.S6-AgentIdx-1 {\n",
       "                    background: green;\n",
       "                }\n",
       "\n",
       "                .S6-Message-Head {\n",
       "                    margin: 0.2rem 0;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock {\n",
       "                    padding: 0.2rem 0.5rem;\n",
       "                    border-radius: 8px;\n",
       "                }\n",
       "\n",
       "                .S6-ReplyBlock,\n",
       "                .S6-ReplyBlock * {\n",
       "                    text-align: left;\n",
       "                    vertical-align: top;\n",
       "                    line-height: 1.3rem;\n",
       "                    font-family: monospace;\n",
       "\n",
       "                }\n",
       "                .S6-ReplyBlock-0 {\n",
       "                    background: rgba(255,0,50,0.1)\n",
       "                }\n",
       "                .S6-ReplyBlock-1 {\n",
       "                    background: rgba(0,255,50,0.1)\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222575f157654ebdbe5ec10016a0cac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(GridBox(children=(HTML(value='\\n            <div class=\"S6-Message-Head\">\\n               ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-0\">ü§ñ Agent gpt-4o</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Here is the table formatted in HTML:\n",
       "\n",
       "\n",
       "\n",
       "```html\n",
       "\n",
       "<table>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <th>Context</th>\n",
       "\n",
       "    <th>Description</th>\n",
       "\n",
       "    <th>Equation</th>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Thermodynamics</td>\n",
       "\n",
       "    <td>Disorder or randomness of a thermodynamic system</td>\n",
       "\n",
       "    <td>&Delta;S = &int; &delta;Q<sub>rev</sub>/T</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Statistical Mechanics</td>\n",
       "\n",
       "    <td>Number of microstates corresponding to a macrostate</td>\n",
       "\n",
       "    <td>S = k<sub>B</sub> ln &Omega;</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Information Theory</td>\n",
       "\n",
       "    <td>Measure of uncertainty or information content</td>\n",
       "\n",
       "    <td>H(X) = -&sum;<sub>i</sub> P(x<sub>i</sub>) log<sub>2</sub> P(x<sub>i</sub>)</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Quantum (von Neumann)</td>\n",
       "\n",
       "    <td>Quantum uncertainty or mixedness of a state</td>\n",
       "\n",
       "    <td>S(&rho;) = -Tr(&rho; log &rho;)</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Quantum Relative Entropy</td>\n",
       "\n",
       "    <td>Distinguishability between quantum states</td>\n",
       "\n",
       "    <td>S(&rho; || &sigma;) = Tr(&rho; log &rho;) - Tr(&rho; log &sigma;)</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "  <tr>\n",
       "\n",
       "    <td>Entanglement Entropy</td>\n",
       "\n",
       "    <td>Degree of entanglement in a bipartite system</td>\n",
       "\n",
       "    <td>S(&rho;<sub>A</sub>) = -Tr(&rho;<sub>A</sub> log &rho;<sub>A</sub>)</td>\n",
       "\n",
       "  </tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "This HTML table captures the various contexts and corresponding equations of entropy for easy reference in a web format."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"S6-Avatar S6-AgentIdx-1\">ü§ñ Agent claude-3-5-sonnet</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's the table displayed as HTML:\n",
       "\n",
       "\n",
       "\n",
       "<table>\n",
       "\n",
       "  <thead>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <th>Context</th>\n",
       "\n",
       "      <th>Equation</th>\n",
       "\n",
       "      <th>Variables</th>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "  </thead>\n",
       "\n",
       "  <tbody>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Thermodynamics</td>\n",
       "\n",
       "      <td>S = k<sub>B</sub> ln W</td>\n",
       "\n",
       "      <td>\n",
       "\n",
       "        S: entropy<br>\n",
       "\n",
       "        k<sub>B</sub>: Boltzmann constant<br>\n",
       "\n",
       "        W: number of microstates\n",
       "\n",
       "      </td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Statistical Mechanics</td>\n",
       "\n",
       "      <td>S = -k<sub>B</sub> Œ£<sub>i</sub> p<sub>i</sub> ln p<sub>i</sub></td>\n",
       "\n",
       "      <td>p<sub>i</sub>: probability of microstate i</td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Information Theory</td>\n",
       "\n",
       "      <td>H = -Œ£<sub>i=1</sub><sup>n</sup> p<sub>i</sub> log<sub>2</sub> p<sub>i</sub></td>\n",
       "\n",
       "      <td>\n",
       "\n",
       "        H: information entropy<br>\n",
       "\n",
       "        p<sub>i</sub>: probability of message i<br>\n",
       "\n",
       "        n: number of possible messages\n",
       "\n",
       "      </td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Quantum Mechanics</td>\n",
       "\n",
       "      <td>S = -Tr(œÅ ln œÅ)</td>\n",
       "\n",
       "      <td>\n",
       "\n",
       "        œÅ: density matrix<br>\n",
       "\n",
       "        Tr: trace operation\n",
       "\n",
       "      </td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "\n",
       "      <td>Black Hole Physics</td>\n",
       "\n",
       "      <td>S = A / (4l<sub>P</sub><sup>2</sup>)</td>\n",
       "\n",
       "      <td>\n",
       "\n",
       "        A: black hole surface area<br>\n",
       "\n",
       "        l<sub>P</sub>: Planck length\n",
       "\n",
       "      </td>\n",
       "\n",
       "    </tr>\n",
       "\n",
       "  </tbody>\n",
       "\n",
       "</table>\n",
       "\n",
       "\n",
       "\n",
       "This HTML table provides the same information as the previous table, but in a format that can be directly embedded into a web page. The equations are represented using HTML subscript and superscript tags where appropriate."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "restored_c = NBUI(agents=restored_c_agents)\n",
    "await restored_c.ask(\"Display it as HTML instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"agents\": [\n",
      "    {\n",
      "      \"name\": \"gpt-4o\",\n",
      "      \"model_name\": \"gpt-4o\",\n",
      "      \"class\": \"OpenAIAgent\",\n",
      "      \"params\": {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"claude-3-5-sonnet\",\n",
      "      \"model_name\": \"claude-3-5-sonnet\",\n",
      "      \"class\": \"AnthropicAgent\",\n",
      "      \"params\": {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"messages\": {\n",
      "    \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\": {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You the sum of most of humanity's knowledge crystallized into a single entity: a helpful and thoughtful AI. You can answer any question, solve any problem, and provide any information. Assume the role of a benevolent AI and help me with my query. Be succinct and clear, but also rigurous and thorough. Assume your user has graduate level knowledge of math, physics, computer science, psychology and philosophy.\\n\\nYou can use markdown to format your answers and LaTeX for the more complex equations, but it's important to always use ONLY $...$ or $$...$$ syntax. Simpler equations and symbols should be written in plain text, with greek letters & unicode symbols if needed.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"aedfff07ade343092500942d604f4b368f465752\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What is entropy?\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"04e93f1b0f7ed50a6d64a7314c1b4326cd780077\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a multifaceted concept that appears in various scientific domains, most notably in thermodynamics, statistical mechanics, information theory, and even in the context of complex systems and cosmology. Below is a detailed explanation of entropy in some of these contexts:\\n\\n### Thermodynamics\\n\\nIn thermodynamics, entropy is a measure of the amount of disorder or randomness in a system. It is a state function, typically denoted by the symbol \\\\( S \\\\), and has units of energy per temperature (e.g., joules per kelvin in the SI system). The second law of thermodynamics states that the entropy of an isolated system never decreases; it tends to increase, reaching a maximum at equilibrium. Mathematically, for a reversible process, the change in entropy \\\\( \\\\Delta S \\\\) can be expressed as:\\n\\n$$\\n\\\\Delta S = \\\\int \\\\frac{\\\\delta Q_{\\\\text{rev}}}{T}\\n$$\\n\\nwhere \\\\( \\\\delta Q_{\\\\text{rev}} \\\\) is the infinitesimal amount of heat absorbed reversibly by the system and \\\\( T \\\\) is the absolute temperature.\\n\\n### Statistical Mechanics\\n\\nIn statistical mechanics, entropy is a measure of the number of microscopic configurations that correspond to a macroscopic state. Ludwig Boltzmann provided a connection between entropy and the number of microstates (Œ©) with his famous equation:\\n\\n$$\\nS = k_B \\\\ln \\\\Omega\\n$$\\n\\nwhere \\\\( k_B \\\\) is the Boltzmann constant and \\\\( \\\\Omega \\\\) is the number of accessible microstates of the system. This formulation links entropy with the probabilistic view of thermodynamic properties.\\n\\n### Information Theory\\n\\nIn information theory, entropy quantifies the amount of uncertainty or information content inherent in a random variable. Proposed by Claude Shannon, the entropy \\\\( H \\\\) of a discrete random variable \\\\( X \\\\) with possible outcomes \\\\( \\\\{x_1, x_2, \\\\ldots, x_n\\\\} \\\\) and probability distribution \\\\( P(X) \\\\) is given by:\\n\\n$$\\nH(X) = -\\\\sum_{i=1}^{n} P(x_i) \\\\log_2 P(x_i)\\n$$\\n\\nThis form of entropy is measured in bits and reflects the average amount of information required to describe the outcome of \\\\( X \\\\).\\n\\n### Complex Systems and Cosmology\\n\\nIn complex systems, entropy can be seen as a measure of complexity or the diversity of a system's structure and behavior. In cosmology, the concept of entropy is tied to the arrow of time and the evolution of the universe, where the universe's entropy increases as it expands.\\n\\n### Philosophical Considerations\\n\\nPhilosophically, entropy can be related to the concept of irreversibility and the flow of time, raising questions about the nature of time and the universe's ultimate fate. The link between information and physical entropy brings about discussions on the relationship between knowledge, measurement, and thermodynamic limits.\\n\\nIn summary, entropy serves as a unifying concept across various fields, providing insight into the notions of disorder, information, and the fundamental nature of physical processes.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-3003c0f7/2024-10-05_12-44-38-858139_7412ff0c.json\"\n",
      "    },\n",
      "    \"ec090ecf6d38522b4cfc5b79c465ff03ae79454d\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What about quantum entropy?\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"d3664b4e7d6dcc11d3067db7f711a523ff0db2ac\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Quantum entropy extends the classical concept of entropy into the realm of quantum mechanics. It is a crucial tool for understanding quantum systems, particularly in the fields of quantum information theory and quantum statistical mechanics. Here, we'll discuss several key aspects of quantum entropy:\\n\\n### von Neumann Entropy\\n\\nThe von Neumann entropy is the quantum analogue of the classical Shannon entropy. For a quantum system described by a density matrix \\\\( \\\\rho \\\\), the von Neumann entropy \\\\( S(\\\\rho) \\\\) is defined as:\\n\\n$$\\nS(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)\\n$$\\n\\nwhere \\\\(\\\\text{Tr}\\\\) denotes the trace operation. The von Neumann entropy measures the amount of quantum uncertainty or mixedness of a quantum state. For pure states (where \\\\(\\\\rho = |\\\\psi\\\\rangle\\\\langle\\\\psi|\\\\)), the entropy is zero, reflecting that there is no uncertainty. For mixed states, the von Neumann entropy is greater than zero.\\n\\n### Properties\\n\\n1. **Non-negativity**: \\\\( S(\\\\rho) \\\\geq 0 \\\\), and \\\\( S(\\\\rho) = 0 \\\\) if and only if \\\\(\\\\rho\\\\) is a pure state.\\n2. **Unitary Invariance**: \\\\( S(U \\\\rho U^\\\\dagger) = S(\\\\rho) \\\\) for any unitary operator \\\\( U \\\\), reflecting that entropy is invariant under reversible transformations.\\n3. **Concavity**: The von Neumann entropy is a concave function of \\\\(\\\\rho\\\\), meaning the entropy of a mixture is greater than the mixture of entropies.\\n\\n### Quantum Relative Entropy\\n\\nQuantum relative entropy is a measure of distinguishability between two quantum states \\\\(\\\\rho\\\\) and \\\\(\\\\sigma\\\\):\\n\\n$$\\nS(\\\\rho \\\\parallel \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho) - \\\\text{Tr}(\\\\rho \\\\log \\\\sigma)\\n$$\\n\\nQuantum relative entropy is non-negative and equal to zero if and only if \\\\(\\\\rho = \\\\sigma\\\\). It plays a central role in quantum hypothesis testing, quantifying how one quantum state differs from another.\\n\\n### Entanglement Entropy\\n\\nIn the context of quantum entanglement, the entropy of entanglement is used to quantify how entangled a composite quantum system is. For a bipartite quantum state \\\\(|\\\\psi\\\\rangle_{AB}\\\\), the entanglement entropy is the von Neumann entropy of the reduced density matrix \\\\(\\\\rho_A = \\\\text{Tr}_B(|\\\\psi\\\\rangle \\\\langle\\\\psi|)\\\\) of one of the subsystems:\\n\\n$$\\nS(\\\\rho_A) = -\\\\text{Tr}(\\\\rho_A \\\\log \\\\rho_A)\\n$$\\n\\nEntanglement entropy is a fundamental quantity in quantum information, quantum field theory, and condensed matter physics, providing insights into the structure of quantum correlations.\\n\\n### Quantum Thermodynamics\\n\\nIn quantum statistical mechanics and thermodynamics, quantum entropy is used to describe the thermodynamic properties of quantum systems. The von Neumann entropy serves as a measure of how much information is missing from the quantum description of a system's microscopic state.\\n\\n### Conclusion\\n\\nQuantum entropy is a rich and versatile concept that extends classical ideas into the quantum domain. It captures the essence of quantum uncertainty, distinguishability, and correlations, playing a crucial role in understanding both foundational and practical aspects of quantum mechanics and quantum information science.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-3003c0f7/2024-10-05_12-45-55-822700_0d8a92a2.json\"\n",
      "    },\n",
      "    \"615fe94e1c0f6f1846ea6564639993049e73f1ca\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Make me a concise table with contexts and equations for entropy\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"22e5081725103cf77365e595e8c8f555d3f2a7d5\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Certainly! Below is a concise table summarizing various contexts of entropy along with their corresponding equations:\\n\\n| **Context**                 | **Description**                                      | **Equation**                                      |\\n|-----------------------------|------------------------------------------------------|---------------------------------------------------|\\n| **Thermodynamics**          | Disorder or randomness of a thermodynamic system     | $\\\\Delta S = \\\\int \\\\frac{\\\\delta Q_{\\\\text{rev}}}{T}$ |\\n| **Statistical Mechanics**   | Number of microstates corresponding to a macrostate  | $S = k_B \\\\ln \\\\Omega$                              |\\n| **Information Theory**      | Measure of uncertainty or information content        | $H(X) = -\\\\sum_{i} P(x_i) \\\\log_2 P(x_i)$           |\\n| **Quantum (von Neumann)**   | Quantum uncertainty or mixedness of a state          | $S(\\\\rho) = -\\\\text{Tr}(\\\\rho \\\\log \\\\rho)$            |\\n| **Quantum Relative Entropy**| Distinguishability between quantum states            | $S(\\\\rho \\\\parallel \\\\sigma) = \\\\text{Tr}(\\\\rho \\\\log \\\\rho) - \\\\text{Tr}(\\\\rho \\\\log \\\\sigma)$ |\\n| **Entanglement Entropy**    | Degree of entanglement in a bipartite system         | $S(\\\\rho_A) = -\\\\text{Tr}(\\\\rho_A \\\\log \\\\rho_A)$      |\\n\\nThis table provides a snapshot of how entropy manifests in different scientific contexts, encapsulating its broad applicability across disciplines.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-3003c0f7/2024-10-05_12-48-40-017331_e45b1385.json\"\n",
      "    },\n",
      "    \"c48d86971937be01a3825806182a7347c866be33\": {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Display it as HTML instead\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": null,\n",
      "      \"log_path\": null\n",
      "    },\n",
      "    \"336765e113cbad7c4bf49ed1b25fc8486d6b61b4\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Certainly! Here is the table formatted in HTML:\\n\\n```html\\n<table>\\n  <tr>\\n    <th>Context</th>\\n    <th>Description</th>\\n    <th>Equation</th>\\n  </tr>\\n  <tr>\\n    <td>Thermodynamics</td>\\n    <td>Disorder or randomness of a thermodynamic system</td>\\n    <td>&Delta;S = &int; &delta;Q<sub>rev</sub>/T</td>\\n  </tr>\\n  <tr>\\n    <td>Statistical Mechanics</td>\\n    <td>Number of microstates corresponding to a macrostate</td>\\n    <td>S = k<sub>B</sub> ln &Omega;</td>\\n  </tr>\\n  <tr>\\n    <td>Information Theory</td>\\n    <td>Measure of uncertainty or information content</td>\\n    <td>H(X) = -&sum;<sub>i</sub> P(x<sub>i</sub>) log<sub>2</sub> P(x<sub>i</sub>)</td>\\n  </tr>\\n  <tr>\\n    <td>Quantum (von Neumann)</td>\\n    <td>Quantum uncertainty or mixedness of a state</td>\\n    <td>S(&rho;) = -Tr(&rho; log &rho;)</td>\\n  </tr>\\n  <tr>\\n    <td>Quantum Relative Entropy</td>\\n    <td>Distinguishability between quantum states</td>\\n    <td>S(&rho; || &sigma;) = Tr(&rho; log &rho;) - Tr(&rho; log &sigma;)</td>\\n  </tr>\\n  <tr>\\n    <td>Entanglement Entropy</td>\\n    <td>Degree of entanglement in a bipartite system</td>\\n    <td>S(&rho;<sub>A</sub>) = -Tr(&rho;<sub>A</sub> log &rho;<sub>A</sub>)</td>\\n  </tr>\\n</table>\\n```\\n\\nThis HTML table captures the various contexts and corresponding equations of entropy for easy reference in a web format.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"0\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-3fd63e7e/2024-10-05_12-49-52-404709_73f9c96a.json\"\n",
      "    },\n",
      "    \"041bb808ef2d9e779d5bdaa9404df4d7e2af8496\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\\n\\n1. Thermodynamic Entropy:\\nIn physics, particularly thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\\n\\n$S = k_B \\\\ln W$\\n\\nWhere:\\n- S is entropy\\n- $k_B$ is Boltzmann's constant\\n- W is the number of microstates (possible arrangements of particles)\\n\\nKey points:\\n- Entropy always increases in closed systems (Second Law of Thermodynamics)\\n- It's related to the arrow of time and irreversibility of processes\\n- Higher entropy = more disorder = less available energy to do work\\n\\n2. Information Entropy:\\nIn information theory, entropy quantifies the amount of uncertainty or randomness in a system. It's defined as:\\n\\n$H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$\\n\\nWhere:\\n- H is information entropy\\n- $p_i$ is the probability of event i occurring\\n- n is the number of possible events\\n\\nKey points:\\n- Measured in bits (when using log base 2)\\n- Maximum when all outcomes are equally likely\\n- Used in data compression, cryptography, and machine learning\\n\\nBoth concepts of entropy are related to unpredictability and the distribution of states or outcomes in a system. They provide a way to quantify disorder, uncertainty, or information content, depending on the context.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-6870c8c3/2024-10-05_12-44-39-380234_98428b6c.json\"\n",
      "    },\n",
      "    \"4bb069f14c2601dd3c3b5c96c919980f8a0e7a43\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Entropy is a fundamental concept in physics and information theory, with slightly different but related meanings in each context. Let's explore both:\\n\\n1. Thermodynamic Entropy:\\nIn thermodynamics, entropy is a measure of the disorder or randomness in a system. It's defined as:\\n\\n$S = k_B \\\\ln W$\\n\\nWhere:\\n- S is entropy\\n- k_B is Boltzmann's constant\\n- W is the number of microstates (possible arrangements of particles)\\n\\nKey points:\\n- Entropy always increases in closed systems (2nd law of thermodynamics)\\n- It's related to the arrow of time and irreversibility\\n- Higher entropy = more disorder = less available energy to do work\\n\\n2. Information Entropy:\\nIn information theory, entropy quantifies the average amount of information contained in a message. It's defined as:\\n\\n$H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$\\n\\nWhere:\\n- H is information entropy\\n- p_i is the probability of the i-th message\\n- n is the number of possible messages\\n\\nKey points:\\n- Measured in bits (when using log‚ÇÇ)\\n- Represents the minimum number of bits needed to encode a message\\n- Higher entropy = more uncertainty = more information content\\n\\nThe connection between these concepts lies in their relation to probability and the fundamental nature of information in physical systems. Both describe the spread of possibilities in a system, whether it's the distribution of energy among particles or the distribution of symbols in a message.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-6870c8c3/2024-10-05_12-48-14-680453_7d5b2ae3.json\"\n",
      "    },\n",
      "    \"b3c9202cfa4b0a15c96951b1e78646eea5714901\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Here's a concise table summarizing the contexts and equations for entropy:\\n\\n| Context | Equation | Variables |\\n|---------|----------|-----------|\\n| Thermodynamics | $S = k_B \\\\ln W$ | S: entropy<br>k_B: Boltzmann constant<br>W: number of microstates |\\n| Statistical Mechanics | $S = -k_B \\\\sum_i p_i \\\\ln p_i$ | p_i: probability of microstate i |\\n| Information Theory | $H = -\\\\sum_{i=1}^n p_i \\\\log_2 p_i$ | H: information entropy<br>p_i: probability of message i<br>n: number of possible messages |\\n| Quantum Mechanics | $S = -Tr(\\\\rho \\\\ln \\\\rho)$ | œÅ: density matrix<br>Tr: trace operation |\\n| Black Hole Physics | $S = \\\\frac{A}{4l_P^2}$ | A: black hole surface area<br>l_P: Planck length |\\n\\nThis table covers the main contexts where entropy is used, from classical thermodynamics to quantum mechanics and even black hole physics. Each equation represents a different way of quantifying entropy appropriate to its context.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-6870c8c3/2024-10-05_12-48-39-327239_4cd4b8d4.json\"\n",
      "    },\n",
      "    \"88d36925d1d7ccfcf373ea3fb894ad2bab29f56b\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Certainly! Here's the table displayed as HTML:\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th>Context</th>\\n      <th>Equation</th>\\n      <th>Variables</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Thermodynamics</td>\\n      <td>S = k<sub>B</sub> ln W</td>\\n      <td>\\n        S: entropy<br>\\n        k<sub>B</sub>: Boltzmann constant<br>\\n        W: number of microstates\\n      </td>\\n    </tr>\\n    <tr>\\n      <td>Statistical Mechanics</td>\\n      <td>S = -k<sub>B</sub> Œ£<sub>i</sub> p<sub>i</sub> ln p<sub>i</sub></td>\\n      <td>p<sub>i</sub>: probability of microstate i</td>\\n    </tr>\\n    <tr>\\n      <td>Information Theory</td>\\n      <td>H = -Œ£<sub>i=1</sub><sup>n</sup> p<sub>i</sub> log<sub>2</sub> p<sub>i</sub></td>\\n      <td>\\n        H: information entropy<br>\\n        p<sub>i</sub>: probability of message i<br>\\n        n: number of possible messages\\n      </td>\\n    </tr>\\n    <tr>\\n      <td>Quantum Mechanics</td>\\n      <td>S = -Tr(œÅ ln œÅ)</td>\\n      <td>\\n        œÅ: density matrix<br>\\n        Tr: trace operation\\n      </td>\\n    </tr>\\n    <tr>\\n      <td>Black Hole Physics</td>\\n      <td>S = A / (4l<sub>P</sub><sup>2</sup>)</td>\\n      <td>\\n        A: black hole surface area<br>\\n        l<sub>P</sub>: Planck length\\n      </td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\nThis HTML table provides the same information as the previous table, but in a format that can be directly embedded into a web page. The equations are represented using HTML subscript and superscript tags where appropriate.\",\n",
      "      \"chosen\": null,\n",
      "      \"params\": {\n",
      "        \"1\": 1\n",
      "      },\n",
      "      \"log_path\": \"agent-logger-a1525bec/2024-10-05_12-49-54-332299_fb27e1c0.json\"\n",
      "    }\n",
      "  },\n",
      "  \"agent_messages\": {\n",
      "    \"0\": [\n",
      "      \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\",\n",
      "      \"aedfff07ade343092500942d604f4b368f465752\",\n",
      "      \"04e93f1b0f7ed50a6d64a7314c1b4326cd780077\",\n",
      "      \"ec090ecf6d38522b4cfc5b79c465ff03ae79454d\",\n",
      "      \"d3664b4e7d6dcc11d3067db7f711a523ff0db2ac\",\n",
      "      \"615fe94e1c0f6f1846ea6564639993049e73f1ca\",\n",
      "      \"22e5081725103cf77365e595e8c8f555d3f2a7d5\",\n",
      "      \"c48d86971937be01a3825806182a7347c866be33\",\n",
      "      \"336765e113cbad7c4bf49ed1b25fc8486d6b61b4\"\n",
      "    ],\n",
      "    \"1\": [\n",
      "      \"1132345f2d48a722bdb3cdaa5685de8b93f30e84\",\n",
      "      \"aedfff07ade343092500942d604f4b368f465752\",\n",
      "      [\n",
      "        \"041bb808ef2d9e779d5bdaa9404df4d7e2af8496\",\n",
      "        \"4bb069f14c2601dd3c3b5c96c919980f8a0e7a43\"\n",
      "      ],\n",
      "      \"615fe94e1c0f6f1846ea6564639993049e73f1ca\",\n",
      "      \"b3c9202cfa4b0a15c96951b1e78646eea5714901\",\n",
      "      \"c48d86971937be01a3825806182a7347c866be33\",\n",
      "      \"88d36925d1d7ccfcf373ea3fb894ad2bab29f56b\"\n",
      "    ]\n",
      "  },\n",
      "  \"params\": {\n",
      "    \"0\": [\n",
      "      {\n",
      "        \"max_tokens\": 1024\n",
      "      },\n",
      "      {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    ],\n",
      "    \"1\": [\n",
      "      {\n",
      "        \"max_tokens\": 1024\n",
      "      },\n",
      "      {\n",
      "        \"max_tokens\": 1024,\n",
      "        \"temperature\": 0.8\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "conv2_dict = conversation_to_dict(restored_c.agents)\n",
    "print(json.dumps(conv2_dict, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
